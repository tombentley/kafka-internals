[[group-protocol,group protocol]]
# Group Protocol

Kafka has a separate group membership protocol and group state protocol.

For the consumer, the group state is basically the assignment of group members to partitions.

For the transaction, TODO

The group protocol is also used for KafkaConnect task assignment and in Kafka Streams.

This protocol is rather complex, but it allows the client to have control over the group state, rather than having to build this into the broker. The protocol is generic, allowing the same basic protocol to be used for consumers and Kafka Connect, but that means that the coordinator doesn't understand the group state and therefore:

* Cannot validate offsets are being committed by the member which was assigned that partition, requiring that clients are bug-free in this respect.
* Cannot invalidate assignments when the topic's metadata changes, pushing this problem onto the client.

[[coordinator-discovery,coordinator discovery]]
## Coordinator discovery

For each group a single broker, known as the group coordinator, is responsible for managing the group management protocol. Before a client can join a group it needs to discover the group coordinator.

1. The consumer queries a random broker, giving the id of the group it wants to join (or at least, find the coordinator of) in a <<FindCoordinator>> request.
2. The broker receives the <<FindCoordinator>> and determines the coordinator according to
+
[source]
----
partitionId = |hashCode(groupId)| mod numPartitions(topicName)
coordinator = getPartition(topicName, partitionId).leader
----
+
Where the `topicName` is `__consumer_offsets` for the consumer group coordinator and 
`__transaction_state` for the transation group coordinator.

3. Client receives <<FindCoordinator>> response and starts talking to this coordinator.

[[group-membership]]
## Group Membership

1. After a client has <<coordinator-discovery,discovered the group coordinator>>, the client sends the coordinator a <<JoinGroup>> request containing metadata about itself and all the group protocols supported by the client.

2. The coordinator waits for up to the rebalance timeout until all it has received a <<JoinGroup>> request from all the members that were in the previous generation (the expected members) 

3. The coordinator selects a protocol which is supported by all members of the group.

4. The coordinator then selects a leader at random and sends a <<JoinGroup>> response in reply to the pending requests. The <<JoinGroup>> response returned to the leader contains the metadata for all the members of the group. The non-leaders are not sent the group membership. In both cases the <<JoinGroup>> response also contains the id of the leader, ??the selected protocol?? and the group generation.
+
A member which joins but doesn't support any protocols supported by the resit of the group gets an error.

5. The leader uses the membership information for the group state protocol.

## Rebalance

Once the client has joined the group it starts sending regular <<Heartbeat>> requests to the coordinator. If the coordinator does not receive a heartbeat from a client before the session timeout the consumer is considered to be dead. The <<Heartbeat>> responses to the remaining clients will have the `REBALANCE_IN_PROGRESS` error code. Those clients have to rejoin the group within the rebalance timeout (measured from the point at which the failed client's session timed out).

In this way a new generation of the group is started. 

<<KIP-62>>.

[[group-state]]
## Group State 

1. The member sends a <<SyncGroup>> request to the coordinator. If the member is the leader of the group this will include the group state. 

2. The coordinator forwards the members state (provided by the leader) in that members <<SyncGroup>> response.

## Offset commit

In order to avoid commits from zombies, <<OffsetCommit>> requests include the group generation id. The coordinator recieves the <<OffsetCommit>> request and validates the generation id. If the generation is not current then the offsets are not committed.

## State machine


TODO: a diagram

The coordinator maintains a state machine with the following states:

`PreparingRebalance`:: The group is preparing to rebalance.
  Transitions to:
  * `CompletingRebalance` if some members join before the timeout.
  * `Empty` if all members have left the group
  * `Dead` if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>).

`CompletingRebalance`:: The group is awaiting for state assignment (<<SyncGroup>>) from the group leader.
  Transitions to:
  * `Stable` if the coordinator receives a <<SyncGroup>> from the group leader.
  * `PreparingRebalance` if the coordinator receives a <<JoinGroup>> from a new member, or an existing member with new metadata; or if the coordinator receives a <<LeaveGroup>> from an existing member; or if member failure is detected (heartbeat timeout)
  * `Dead` if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>).

`Stable`:: Transitions to:
  * `PreparingRebalance` if the coordinator receives a <<JoinGroup>> from a new member, or an existing member with new metadata; or if the coordinator receives a <<LeaveGroup>> from an existing member; or if member failure is detected (heartbeat timeout)

`Dead`:: 
  There are no active members; the group metadata is being cleaned up.
  

`Empty`::
  The group has no members. The group continues in this state until offsets have expired. Transitions to:
  * `Dead` last offsets removed; or if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>); or if the group is removed by expiration.
  * `PreparingRebalnce` if a new member joins the group. 

## Rebalance latency

TODO

## Coordinator crash

The coordinator stores the group generation, leader etc in the group topic (e.g. `__consumer_offsets` for consumer groups). If the coordinator fails that will be detected (for example, by ZooKeeper) and a new leader of the group topic (and thus a new coordinator) will be elected. 

The clients detect failure of the coordinator because they don't receive a <<Heartbeat>> response within the session timeout, so they find the new coordinator and start sending their heartbeats to it. Because the new coordinator can re-read the group topic to discover the membership, generation etc, there is no need for the clients to perform the group membership or state protocols.

## Embedded protocols

How a group works in a client (Consumer, Kafka Connect, Kafka Streams) depends on the  `AbstractCoordinator` implementation, which is responsible for generating all the group protocol requests, and handling all the responses, used by the client. That is, it:

* generates the  <<JoinGroup>> requests (including the necessary use data for each protocol the client can use)
* generates the <<SyncGroup>> request where the leader decides on the group state.

The Kafka Consumer and Kafka Connect use separate `AbstractCoordinator` implementations,`ConsumerCoordinator` and `WorkerCoordinator` respectively, because their group state differs:

* For the consumer it is a partitioning of the subscribed topics
* For a Connect worker it is a partitioning of tasks

`AbstractCoordinator.metadata()` supplies the _protocols_ which get send in the <<JoinGroup>> request.

For the `ConsumerCoordinator` there's another level of pluggability via the `Assignor`. It's at the `Assignor` level that Kafka Streams differs from the plain Kafka Consumer.
For the `ConsumerCoordinator` it's the `Assignors` which correspond to the protocols in the <<JoinGroup>> request.


https://www.youtube.com/watch?v=QaeXDh12EhE

https://cwiki.apache.org/confluence/display/KAFKA/Incremental+Cooperative+Rebalancing%3A+Support+and+Policies#IncrementalCooperativeRebalancing:SupportandPolicies-Motivation

https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol


https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread