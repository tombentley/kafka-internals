# The Producer

## Motivating questions

. What is the only Kafka RPC request that lacks a response?
. Can a zombie idempotent producer get fenced?
. What producer-related state resides on a partition leader?
. How are zombie producers prevented from writing duplicate records?
. Which broker acts as transaction coordinator?
. What is a control batch used for?
. What logs might be needed to resolve a problem with transactions?


## Overview

Having done the necessary bootstrapping and discovered which brokers are acting as leader for the relevant partitions, a producer is finally in a position to send messages.
This is done using a `PRODUCE` request which essentially contains the messages to be appended to the log for some set of partitions led by that broker.

## The Simple producer

## `PRODUCE`

### `ProduceRequest`

The fields in the produce request should be fairly self-explanatory.

[id=ProduceRequest]
[source,javascript]
.The `PRODUCE` request
----
include::../kafka-sources/clients/src/main/resources/common/message/ProduceRequest.json[lines=16..-1]
----

### `ProduceResponse`

When the produce request has `acks=1` or `acks=all` the broker will response with a `ProduceResponse` once all the records have been appended to its, and for `acks=all` enough follower, logs.

NOTE: `acks=0` is a special case in the Kafka protocol: It's the only request which doesn't result in a response being returned.
That is, `acks=0` doesn't just mean the producer doesn't _wait_ for an acknowledgement, it means there is not acknowledgement at all.

[id=ProduceResponse]
[source,javascript]
.The `PRODUCE` response
----
include::../kafka-sources/clients/src/main/resources/common/message/ProduceResponse.json[lines=16..-1]
----

Of course there's the possibility that the broker is not the leader for some of the partitions in the request.
In this case the `NOT_LEADER_FOR_PARTITION` error will prompt the producer to refresh it's metadata and resend the records for those partitions to the new leader, if there is one. 
If there is no new leader the messages will eventually expire.

What can be inferred when a `PRODUCE` request is sent but no `PRODUCE` response is received? Nothing! 
The sender has no idea whether the request reached a broker or not, and must choose between resending the message (risking a duplicate message being appended to the log), or not resending (risking loss of the message). 

### The record format
// TODO Record format

**TODO**

## Producer architecture

[source]
....
              +---------------+        +-------------+
    send()--> | KafkaProducer | -----> | Partitioner |
              +---------------+        +-------------+
                |         |
                V         +---------------------+ 
+-------------------+     |                     |
| RecordAccumulator |     |                     |
+-------------------+     |      Â               |
                ^         V                     V
                |    +--------+         +------------------+
                +--- | Sender | ------> | ProducerMetadata |
                     +--------+         +------------------+
                          |
                          V
                  +---------------+
                  | NetworkClient | --> to network
                  +---------------+
....

// This is a bit of a sketch, and misses out important details, like how METADATA requests actually get sent
// and transactions

When `send()` is called, on the application thread, the following happens:

. Any configured interceptors are run
. Fetch topic metadata if necessary (`ProducerMetadata`)
. The key and value are serialized
. The `Partitioner` assigns a partition
. The compressed size of the record is estimated
. The record may be added to the last batch in the `Accumulator`
. If the last batch is full an new batch is created
.. The `Partitioner` _reassigns_ the partition
.. `Accumulator`: Append to new batch
. Wake the `Sender` if the batch last batch is now full or a new batch was created

The `Sender` is running continually on the IO thread:

. Ask the accumulator which the brokers for which there are ready batches
. If there are any topics with an unknown leader, request metadata update
. Filter out the brokers where the network is not ready for sending requests
. Drain batches from the `Accumulator` for each ready broker
. Create `PRODUCE` requests and send them to the `NetworkClient`

The `RecordAccumulator` uses a deque:
* The application thread added to the back of the dequeue
* The IO threads removes from the front of the deque
* Failed sends are pushed back on the front of the deque, so that message order is maintained.

## The idempotent producer

Support for idempotent producers was added in KIP-98.

A `ProduceRequest` is not, on its own, idempotent.
In particular, if the client doesn't receive a response it may resend a produce request, resulting in duplicate records in the log. 
It is made idempotent by adding a sequence number to each record prior to sending it the first time and having the broker keep track of the sequence number of the last appended record in each partition. 
If the broker observes a new record batch whose sequence number is less than or equal to this last appended record it doesn't append the record, but acknowledges it back to the client anyway (since that records must already be in the log).
Likewise if the broker observes a batch whose sequence number is larger than the next expected number is will also be rejected, because there must be another batch, as-yet unobserved, which needs to be appended _first_.

The observed sequence number needs to be per-producer and its lifetime is that of the producer process (not its connection to the broker). 
The broker doesn't otherwise know when a producer process or session ends, so there needs to be an explicit way for the broker to identify produce requests from the same producer session.
This is the purpose of the producer id.

### Obtaining an idempotent Producer Id (PID) (`INIT_PRODUCER_ID`)

The producer asks for an identifier from any broker and sends this in each record along with its sequence number for that partition.
The sequence is basically tied to that PID, so if the producer restarts it will request a PID, be granted a new one and start the sequence again from zero.
// Is it zero?

#### `InitProducerIdRequest`

The exact behaviour depends on whether the producer is idempotent (i.e. does not have `transactional.id` set) or transactional.

We'll defer discussion of the transactional case till later, for now let's focus on the idempotent case.
The producer sends an `InitProducerIdRequest` to a random broker, which allocates an id, which is guaranteed to be unique. 
The producer epoch will always be zero since PIDs for idempotent producers can only have a single epoch, since they ephemeral like the producer process.


[id=InitProducerIdRequest]
[source,javascript]
.The `INIT_PRODUCER_ID` request
----
include::../kafka-sources/clients/src/main/resources/common/message/InitProducerIdRequest.json[lines=16..-1]
----

#### `InitProducerIdResponse`

[id=InitProducerIdResponse]
[source,javascript]
.The `INIT_PRODUCER_ID` response
----
include::../kafka-sources/clients/src/main/resources/common/message/InitProducerIdResponse.json[lines=16..-1]
----

The PID and epoch are returned to the producer, which will use them for every `PRODUCE` request made.

.Example sequence of RPCs for an idempotent producer
[seqdiag, fontpath="/usr/share/fonts/truetype/inconsolata/Inconsolata-Bold.ttf"]
....
seqdiag {
  default_fontsize = 16;
  autonumber = True;
  edge_length = 400;  // default value is 192
  span_height = 15;  // default value is 40
  producer  -> "any broker" [label = "INIT_PRODUCER_ID TransactionalId=null"];
  producer <-- "any broker" [label = "ProducerId=1234 ProducerEpoch=0"];
  producer  -> "leader(topic,0)" [label = "PRODUCE Records=(ProducerId=1234 ProducerEpoch=0, Sequence=1, ...)"];
  producer <-- "leader(topic,0)";
  producer  -> "leader(topic,0)" [label = "PRODUCE Records=(ProducerId=1234 ProducerEpoch=0, Sequence=2, ...)"];
  producer <-- "leader(topic,0)";
}
....

You might recall that the schema for `ProduceRequest` doesn't include fields for the producer id or epoch.
That's because they're actually part of the v2 message format, so are present in the request embedded within the `Record` field.
Also present in the v2 message format is a sequence number. 
The producer is careful to allocate the sequence number once for each batch.
That is, if the `ProduceRequest` has to be resent, for example because the partition leadership changed, the batch has the same sequence number.

The leader keeps a map of PID to last-appended sequence number (one map for each partition).
This means it knows what sequence number to expect next for a given producer id. 

* If `actual_sequence_number <= last_appended` if knows that this batch has already been appended, and so it doesn't need to append it again, and returns `DuplicateSequenceNumber` to the producer. The producer can ignore this error code. 
* If the `actual_sequence_number < last_appended + 1` it knows that a batch must have gone missing. In the absence of log truncation this should never happen, so the leader returns `InvalidSequenceNumber` so the producer, which treats it as a fatal error.
* Only if `actual_sequence_number < last_appended + 1` will the leader append to the log.

What can be inferred when an idempotent `PRODUCE` request is sent but no `PRODUCE` response is received? Still nothing! 
But crucially the decision about whether to resend is easy: Resending doesn't risk duplicate messages being appended to the log.

## The transactional producer

Support for transactions was also added in KIP-98.

This allows a producer to send messages to a set of partitions (with leaders on multiple brokers) within a transaction that is committed or aborted atomically. It also guarantees atomicity when sending multiple produce requests to the same broker.
"Atomic" means either all the messages in the transaction become visible to (suitably configured) consumers (if the transaction was committed) or none of them do (if the transaction was aborted).
A transactional producer is also idempotent.

Because logs are immutable and records get appended during the transaction and not at the end, it is necessary for transactional consumers (i.e. those with an isolation level of `read_committed`) to buffer incomplete transactions in memory. Special control records, called "end transaction markers" or "commit markers", are present in the log to mark the end of a transaction. If the marker shows the transaction was aborted the buffered records are silently dropped by the client library and not passed on to the application.

The producer uses a number of additional requests to achieve this. 
Extra interbroker communication is also required.

1. The producer must find the broker which is acting as its transaction coordinator.

2. The producer gets a _transactional_ producer id from its coordinator. Unlike a PID for a purely idempotent producer, a transactional PID identifies the producer across multiple sessions.

3. The producer sends produce requests to partition leaders (typically other brokers than the coordinator), and tells its coordinator which partitions and offsets it is interacting with.

4. The producer send and end transaction message to its coordinator, either committing it, or rolling it back.

The producer than then repeat steps 3 to 4 indefinitely, or until the producer becomes fenced. 

.Example sequence of RPCs for a transaction producer (interbroker RPCs not shown)
[seqdiag, fontpath="/usr/share/fonts/truetype/inconsolata/Inconsolata-Bold.ttf"]
....
seqdiag {
  default_fontsize = 16;
  autonumber = True;
  edge_length = 400;  // default value is 192
  span_height = 15;  // default value is 40
  producer  -> "any broker" [label = "FIND_COORDINATORS TransactionalId=myId"];
  producer <-- "any broker" [label = "Coordinator=1"];
  producer  -> "txn_coordinator" [label = "INIT_PRODUCER_ID TransactionalId=myId"];
  producer <-- txn_coordinator [label = "ProducerId=1234 ProducerEpoch=2"];
  producer  -> txn_coordinator [label = "ADD_PARTITION_TO_TXN partition=foo-0"];
  producer <-- txn_coordinator;
  producer  -> "leader(foo,0)" [label = "PRODUCE Records=(ProducerId=1234 ProducerEpoch=0, ...)"];
  producer <-- "leader(foo,0)";
  producer  -> txn_coordinator [label = "ADD_PARTITION_TO_TXN partition=bar-1"];
  producer <-- txn_coordinator;
  producer  -> "leader(bar,1)" [label = "PRODUCE Records=(ProducerId=1234 ProducerEpoch=0, ...)"];
  producer <-- "leader(bar,1)";
  producer  -> txn_coordinator [label = "END_TXN commit=true"];
  producer <-- txn_coordinator;
}
....

Complicated as the above diagram is, it's actually a simplification. 
It is not showing the interbroker communication, and the situation for a streaming application it is more complex.

[[coordinator-discovery,coordinator discovery]]
### Coordinator discovery (`FIND_COORDINATOR`)

NOTE: Coordinator discovery is used for both transactional producers and consumer groups, so in this section we'll talk about the client, rather than the producer. We'll refer back to this section later then discussing consumer groups.

Coordinator discovery is about unambiguously identifying a unique broker based on some identity. 
For the transactional producer that identity is its `transactional.id`. 
For a member of a consumer group that identity is its `group.id`.

1. The client queries a random broker, giving the required coordinator type and the client's identity in a `FIND_COORDINATORS` request
2. The random broker receives the request and determines the coordinator according to
+
[source]
----
// Pseudocode
partitionId = abs(hashCode(identity)) mod numPartitions(topicName)
coordinator = leaderOf(topicName, partitionId)
----
+
Where the `topicName` is `\__consumer_offsets` for the consumer group coordinator and `__transaction_state` for the transaction coordinator. 
In other words, the leaders of partitions of those topics have an additional role on top of their leadership.
They have to manage extra broker-side state for the clients they're coordinating.

3. The client receives a `FIND_COORDINATOR` response and starts talking to this coordinator.

#### `FindCoordinatorRequest`

[id=FindCoordinatorRequest]
[source,javascript]
.The `FIND_COORDINATOR` request
----
include::../kafka-sources/clients/src/main/resources/common/message/FindCoordinatorRequest.json[lines=16..-1]
----

#### `FindCoordinatorResponse`

[id=FindCoordinatorResponse]
[source,javascript]
.The `FIND_COORDINATOR` response
----
include::../kafka-sources/clients/src/main/resources/common/message/FindCoordinatorResponse.json[lines=16..-1]
----

This mechanism ensures that all the state on the broker about that client can be persistect to the log, and log replication ensures that should the broker crash the new leader will have access to exactly the same state.
In this way the transaction and group coordinatorship is a role which can migrate between brokers (because it's tied to partition leadership).

### Obtaining an transactional Producer Id (PID)

This is basically the same `InitProducerIdRequest` we've seen for the idempotent producer, except:

* it must be send to the transaction coordinator,
* the request includes the `transactional.id` and the transaction timeout

.Example sequence of RPCs for obtaining a PID in a transactional producer
[seqdiag, fontpath="/usr/share/fonts/truetype/inconsolata/Inconsolata-Bold.ttf"]
....
seqdiag {
  default_fontsize = 16;
  autonumber = True;
  edge_length = 400;  // default value is 192
  span_height = 15;  // default value is 40
  app       ->> producer [label="initTransactions()"];
  producer  -> "any broker" [label = "FIND_COORDINATORS TransactionalId=myId"];
  app      <<-- producer;
  producer <-- "any broker" [label = "Coordinator=1"];
  producer  -> "txn_coordinator" [label = "INIT_PRODUCER_ID TransactionalId=myId"];
  producer <-- "txn_coordinator" [label = "ProducerId=1234 ProducerEpoch=42"];
  
}
....

The association between the transactional id and its PID is written to the relevant `__transaction_state` partition by the coordinator.
This allows the same PID to be issued to the same producer (as identified by transactional id) even in the event the leader changes (e.g. due to a broker crash) or the producer process restarts. 
Each time a PID is requested for a given transactional id the transaction coordinator will increment the producer epoch associated with it.
The `<PID, Epoch>` pair thus uniquely identify a producer session, and in particular a broker receiving a state from an old instance of the producer application will observe a producer epoch that's smaller than the epoch it's most recently observed and can thus fence our the zombie producer. 

This means that the choice of `transactional.id` is crucial if zombie producers are going to be effectively fenced off.
It needs to identify the "logical producer", and the partitions it will be producing to.
It needs to be stable across restarts of that producer process.
If a pool of producers were being used, _something_ has to guarantee that individual messages can only be sent by one producer from the pool.
If it's possible for an individual message to be sent by producer instances with different transactional ids (possibly as the result of some higher level retry strategy) then duplicates are possible and the transactional semantics are lost.


### Beginning a transaction

There is no explicit RPC for starting a transaction. 
The producer's `beginTransaction()` API method only changes state local to the producer.

### Adding partitions to a transaction (`ADD_PARTITIONS_TO_TXN`)

The first time the producer sends a `ProduceRequest` to a new partition (that's not yet part of this transaction) it will also send an `AddPartitionsToTxnRequest` to the coordinator.


#### `AddPartitionsToTxnRequest`

[id=AddPartitionsToTxnRequest]
[source,javascript]
.The `ADD_PARTITIONS_TO_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/AddPartitionsToTxnRequest.json[lines=16..-1]
----

The coordinator records this state in `__transaction_state` so that when the transaction is ended it knows which brokers need to be sent requests to write transaction markers.

#### `AddPartitionsToTxnResponse`

[id=AddPartitionsToTxnResponse]
[source,javascript]
.The `ADD_PARTITIONS_TO_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/AddPartitionsToTxnResponse.json[lines=16..-1]
----

.Example sequence of RPCs transactional production
[seqdiag, fontpath="/usr/share/fonts/truetype/inconsolata/Inconsolata-Bold.ttf"]
....
seqdiag {
  default_fontsize = 16;
  autonumber = True;
  edge_length = 400;  // default value is 192
  span_height = 15;  // default value is 40
  app       ->> producer [label="send(topic=foo, partition=0)"];
  producer  -> "txn_coordinator" [label = "ADD_PARTITIONS_TO_TXN TransactionalId=myId partition=foo-0"];
  producer  -> "leader(foo,0)" [label = "PRODUCE"];
  app      <<-- producer;
  producer  <-- "leader(foo,0)";
  producer  <-- "txn_coordinator";
}
....

// Example transactional PRODUCE with PID, epoch, sequence number

### Adding offsets to a transaction (`ADD_OFFSETS_TO_TXN`, `TXN_OFFSET_COMMIT`)

NOTE: This section is only relevant for non-streaming applications. For pure-producers you can skip to the following section.

Streaming applications (that is, those where records consumed from one set of partitions cause new records to be written the other partitions) need a way to add the consumer's offsets to the transaction, since such offset commit also results in appending records to the `__consumer_offsets` partition.
In other words, a consumer committing offsets is causing log appends for the transaction scope needs to include those log appends.
If `__consumer_offsets` was not included in the transaction (which means if it didn't get markers written on transaction completion) it would be possible for the downstream partitions to get out of sync with the upstream partitions.
The `AddOffsetsToTxnRequest` is thus the `__consumer_offsets` flavour of the `AddPartitionsToTxnRequest` for a normal topic, and is sent to the _transaction_ coordinator after the `KafkaConsumer.commit`

In the Java API this is achieved by `KafkaProducer.sendOffsetsToTransaction()`. 
This method acts as a bridge between consumer state, and a producers transactional state.
The consumer group state is extracted from the consumer instance using `KafkaConsumer.groupMetadata()` and consists of:

* group id
* generation id
* member id
* group instance id

`KafkaProducer.sendOffsetsToTransaction()` first sends a `AddOffsetsToTxnRequest` to the _transaction_ coordinator, which is handled in essentially exactly the same way as `AddPartitionsToTxn`.
When the producer receives an OK `AddOffsetsToTxnRequestResponse` it will send a `TxnOffsetCommitRequest` to the _group_ coordinator.
The handling is different, but ultimately it calls into the same `GroupMetadataManager.storeOffsets()` method that's used for normal consumer commit (`OffsetCommitRequest`).

#### `AddOffsetsToTxnRequestRequest`

[id=AddOffsetsToTxnRequestRequest]
[source,javascript]
.The `ADD_OFFSETS_TO_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/AddOffsetsToTxnRequest.json[lines=16..-1]
----

As for the `AddPartitionsToTxnRequest`, the coordinator will append a record to the `__transaction_state` partition recording that the producer interacted with `__consumer_offsets`, before sending the response.

#### `AddOffsetsToTxnRequestResponse`

[id=AddOffsetsToTxnRequestResponse]
[source,javascript]
.The `ADD_OFFSETS_TO_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/AddOffsetsToTxnResponse.json[lines=16..-1]
----

#### `TxnOffsetCommitRequest`

The `TxnOffsetCommitRequest` is the transactional flavour of the `OffsetCommitRequest` and is send to the _group_ coordinator.

[id=TxnOffsetCommitRequest]
[source,javascript]
.The `TXN_OFFSET_COMMIT` request
----
include::../kafka-sources/clients/src/main/resources/common/message/TxnOffsetCommitRequest.json[lines=16..-1]
----

#### `TxnOffsetCommitResponse`

[id=TxnOffsetCommitResponse]
[source,javascript]
.The `TXN_OFFSET_COMMIT` response
----
include::../kafka-sources/clients/src/main/resources/common/message/TxnOffsetCommitResponse.json[lines=16..-1]
----

.Example sequence of RPCs for committing offsets in a transactional streaming application
[seqdiag, fontpath="/usr/share/fonts/truetype/inconsolata/Inconsolata-Bold.ttf"]
....
seqdiag {
  default_fontsize = 16;
  autonumber = True;
  edge_length = 400;  // default value is 192
  span_height = 15;  // default value is 40
  app       ->> consumer [label="groupMetadata()"];
  app      <<-- consumer;
  app       ->> producer [label="sendOffsetsToTransaction()"];
  producer  -> "txn_coordinator" [label = "ADD_OFFSETS_TO_TXN TransactionalId=myId"];
  app      <<-- producer;
  producer  <-- "txn_coordinator";
  producer  -> "grp_coordinator" [label = "TXN_OFFSET_COMMIT TransactionalId=myId"];
  producer <-- "grp_coordinator";
}
....

Transactional producers which are not also consumers don't need to use these RPCs, which are driven by use of the `sendOffsets()` API on the client.

### Ending a transaction (`END_TXN`, `WRITE_TXN_MARKERS`)

The producer ends the transaction when the application calls `commitTransaction()` or `abortTransaction()`.
In either case the producer sends a `EndTxnRequest` to the transaction coordinator. 

#### `EndTxnRequest`

[id=EndTxnRequest]
[source,javascript]
.The `END_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/EndTxnRequest.json[lines=16..-1]
----

The coordinator writes another record to `__transaction_state` recording that the producers transaction was ended.
It can then send a response back to the client.

#### `EndTxnResponse`

[id=EndTxnResponse]
[source,javascript]
.The `END_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/EndTxnResponse.json[lines=16..-1]
----


#### `WriteTxnMarkersRequest`
Although from the producer's perspective the transaction is now complete, the transaction coordinator's work is not yet done.
The coordinator uses the accumulated state for the transaction in `__transaction_state` to complete the transaction. 
It does this by sending `WriteTxnMarkersRequests` to each of the brokers leading the partitions added to the transaction, including the `__consumer_offsets` partitions if `sendOffsets()` was used. The transaction markers record whether the transaction was aborted or committed, and will be used later to filter out aborted transactions on consumers with the `read_committed` isolation level.

[id=WriteTxnMarkersRequest]
[source,javascript]
.The `WRITE_TXN_MARKERS` request
----
include::../kafka-sources/clients/src/main/resources/common/message/WriteTxnMarkersRequest.json[lines=16..-1]
----

#### `WriteTxnMarkersResponse`

[id=WriteTxnMarkersResponse]
[source,javascript]
.The `WRITE_TXN_MARKERS` response
----
include::../kafka-sources/clients/src/main/resources/common/message/WriteTxnMarkersResponse.json[lines=16..-1]
----

.Example sequence of RPCs for ending a transaction
[seqdiag, fontpath="/usr/share/fonts/truetype/inconsolata/Inconsolata-Bold.ttf"]
....
seqdiag {
  default_fontsize = 16;
  autonumber = True;
  edge_length = 400;  // default value is 192
  span_height = 15;  // default value is 40
  app       ->> producer [label="commitTransaction()"];
  producer  -> "txn_coordinator" [label = "END_TXN commit=true"];
  producer  <-- "txn_coordinator";
  app      <<-- producer;
  txn_coordinator  -> "leader(foo,0)" [label = "WRITE_TXN_MARKERS"];
  txn_coordinator  -> "leader(bar,1)" [label = "WRITE_TXN_MARKERS"];
  txn_coordinator  -> "leader(__consumer_offsets,12)" [label = "WRITE_TXN_MARKERS"];
  txn_coordinator <-- "leader(bar,1)";
  txn_coordinator <-- "leader(__consumer_offsets,12)";
  txn_coordinator <-- "leader(foo,0)";
}
....



#### HA of the coordinator

Note that by writing to the `__transaction_state` log at every step, the coordinator is able to recover from a broker crash.
If this happens, the leadership of the `__transaction_state` partition will move to a replica, and so the new leader will also become the new coordinator.
The new coordinator has access to the transaction state since it was already replicated via the normal log replication protocol.
In this way the new coordinator is able to pick up where the old coordinator left off. 

// TODO transaction timeout

### The `TransactionManager`

On the producer, transactions are managed by the `TransactionManager`. 
This was left out of the previous diagram

[source]
....
                +---------------+        +-------------+
     send()---> | KafkaProducer | -----> | Partitioner |
                +---------------+        +-------------+
                 |          |  |
                 |          |  +----------------+ 
                 V          V                   |
+-------------------+     +------------+        |
| RecordAccumulator | --> | TxnManager |        |
+-------------------+     +------------+        |
                ^            ^                  |
                |            |                  V
                |    +--------+         +------------------+
                +--- | Sender | ------> | ProducerMetadata |
                     +--------+         +------------------+
                          |
                          V
                  +---------------+
                  | NetworkClient | --> to network
                  +---------------+
....

The transaction manager uses a state machine to determine what transactional requests need to be sent.

.The `TransactionManager` State Machine
[smcat]
....
"Uninit",
Initializing,
Ready,
InTransaction,
CommittingTxn,
AbortingTxn,
AbortableError,
FatalError;

Ready => "Uninit";
"Uninit" => Initializing;
AbortingTxn => Initializing;
Initializing => Ready;
CommittingTxn => Ready;
AbortingTxn => Ready;
Ready => InTransaction;
InTransaction => CommittingTxn;
InTransaction => AbortingTxn;
AbortableError => AbortingTxn;
InTransaction => AbortableError;
CommittingTxn => AbortableError;
AbortableError => AbortableError;
"Uninit" => FatalError;
Initializing => FatalError;
Ready => FatalError;
InTransaction => FatalError;
CommittingTxn => FatalError;
AbortingTxn => FatalError;
AbortableError => FatalError;
....

// TODO write some more about the TransactionManager


## Answers to the motivating questions

What is the only Kafka RPC request that lacks a response?::
`PRODUCE` with `acks=0`
Can a zombie idempotent producer get fenced?::
No, because fencing is based on the `transactional.id`, which isn't set for a purely idempotent producer
What producer-related state resides on a partition leader?::
For an idempotent or transactional producer the producer epoch and sequence number.
How are zombie producers prevented from writing duplicate records?::
The coordinator will increment the epoch for the producer; on observing a higher epoch, leaders will fence out lower epochs.
Which broker acts as transaction coordinator?::
The leader of of the relevant partition of `__transaction_state`
What is a control batch used for?::
Marking whether a transaction was committed or aborted.
What logs might be needed to resolve a problem with transactions?::
The producer's, the transaction coordinator's, the brokers that were leaders for the partitions producers to, and possibly the consumer group coordinator's too for a streaming application.
If there were changes in leadership during the time of the problem the old and new leader's logs could be needed.
Obviously, if the problem is better defined then you can narrow this list down somewhat.
