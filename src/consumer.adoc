# The Consumer

## Motivating questions

. What fetch-related state resides on a partition leader?
. When might a consumer successfully fetch from a partition follower?
. Which broker acts as group coordinator?
. How does a group detect and recover from the loss of (a) the coordinator, (b) a group member?
. How many rebalances are required when a new member joins an established group, and how impactful is than on throughput?


## The simple Consumer

The "simple" consumer here means a consumer which will consume from partitions _assigned_ to it via the API. 
The group consumer (where the consumer is _subscribed_ to topics) will be covered later.

Following the bootstrapping process already described, the simple consumer can start making `FETCH` requests immediately.

### `FETCH`

#### `FetchRequest`

The `FETCH` API is used by both consumers and followers (brokers), and in several different ways.

[id=FetchRequest]
[source,javascript]
.The `FETCH` request
----
include::../kafka-sources/clients/src/main/resources/common/message/FetchRequest.json[lines=16..-1]
----

In the simple consumer case:
1. the consumer makes a fetch request which lists the assigned `Topics/Partition` of interest and the `Topics/Partition/FetchOffset` from which to read
2. it processes the returned data
3. it makes a new fetch request listing (usually) the same partitions, and the offset it computed from the records in the returned data.

Since the offset is explicit, such a fetch request is idempotent.
The broker just zero-copies some data from the log between the requested offset and the high watermark or last stable offset (LSO) of the log to the network.
Because the broker doesn't really _read_ the data and it doesn't know how many records were copied, so it doesn't know what the next offset would be.

// TODO example log for a fetch request

#### `FetchResponse`

[id=FetchResponse]
[source,javascript]
.The `FETCH` response
----
include::../kafka-sources/clients/src/main/resources/common/message/FetchResponse.json[lines=16..-1]
----

// TODO example log for a fetch response

#### Incremental fetch 
Most consumers will request data for the same set of partitions repeatedly, meaning that sending the Partitions of interest with each request is wasteful of network bandwidth.
KIP-227 added support for "incremental fetch sessions" which avoids this overhead.
This makes fetch requests stateful and comes with the cost of some memory on the broker to maintain this broker-side state.
The client starts by making an almost ordinary `FETCH` request, but uses `SessionId=0` and `SessionEpoch=0` to request the creation of a session.
If the broker allows a session to be created the response will contain a strictly positive `SessionId`. 
Most subsequent requests from the client can pass the `SessionId` instead of the whole set of partitions.
Only when the nature of the client's interest in a partition changes will it need to include partition in a later fetch request.

// TODO example log for an incremental fetch request

// TODO discuss impl details of the fetch session cache, e.g. limited size, thread contention on the cache.

// TODO limitations of the fetch protocol (e.g. minimal pipelining being a problem on high latency links with high throughput partitions).

#### Consumer follower fetching

NOTE: The term "follower fetch" is ambiguous.
It could refer to the fetching done by a follower, or fetching from a follower by a consumer.
Here we mean the latter.

Usually consumers will fetch from the partition leader, since that will result in the smallest end-to-end latency. 
However, KIP-392 added support for consumers to fetch from followers. 
This can be beneficial when the consumer is in a different rack to the leader, but the same rack as a follower, and there are significant (financial or performance) costs to fetching between racks.

The basic idea is that the consumer includes `RackId` in its request and the broker's response will include, in addition to the requested data, a `PreferredReadReplica` nominating a broker in that rack to serve future fetch requests. 


### Consumer offset management (`OFFSET_COMMIT` and `OFFSET_FETCH`)

// TODO OFFSET_COMMIT, OFFSET_FETCH)


## Consumer architecture: `Fetcher`


## Group membership protocol

// FIND_COORDINATOR, JOIN_GROUP, SYNC_GROUP, LEAVE_GROUP, HEARTBEAT

The previous sections discussed how the simple consumer works. 
We now go into how Kafka enables _groups_ of consumer processes (called members) to work together.
The design separates group _membership_ and group _state_ into two separate protocols.
Group membership allows a collection of consumer processes with the same `group.id` to dynamically discover each other and share some state. 
It copes with members and brokers coming and going.

### Coordinator discovery (again)
For each group a single broker, known as the group coordinator, is responsible for managing the group management protocol. 
Before a client can join a group it needs to discover the group coordinator. 
This uses the same <<coordinator-discovery>> mechanism described for the transactional producer:
The consumer asks an arbitrary broker for it's coordinator, and the coordinator uses a hash of the `group.id`, to determine a partition of `__consumer_offsets`; the leader of that partition is the coordinator.

[[group-membership]]
### Joining a group

After a client has discovered the group coordinator, the client sends the coordinator a `JOIN_GROUP` request containing metadata about itself and all the group protocols supported by the client.

[id=JoinGroupRequest]
[source,javascript]
.The `JOIN_GROUP` request
----
include::../kafka-sources/clients/src/main/resources/common/message/JoinGroupRequest.json[lines=16..-1]
----

The coordinator waits for up to the rebalance timeout until all it has received a `JOIN_GROUP` request from all the other members.
The coordinator then selects a protocol which is supported by all members of the group.

The coordinator then selects a leader at random and sends a `JOIN_GROUP` response in reply to the pending requests. 
The `JOIN_GROUP` response returned to the leader contains the metadata for all the members of the group.

[id=JoinGroupResponse]
[source,javascript]
.The `JOIN_GROUP` response
----
include::../kafka-sources/clients/src/main/resources/common/message/JoinGroupResponse.json[lines=16..-1]
----

The response to non-leaders does not include the group membership.
In either case the `JOIN_GROUP` response contains the id of the leader, ??the selected protocol?? and the group generation.

A member which joins but doesn't support any protocols supported by the rest of the group gets an error.


### Heartbeating and rebalancing.

Once a client has joined the group it starts sending regular `HEARTBEAT` requests to the coordinator.
This serves several purposes:

* If the coordinator does not receive a heartbeat from a client before the session timeout the consumer is considered to be dead and the coordinator will initiate a rebalance.
* If the coordinator received a `JOIN_GROUP` request from a prospective member since the last time it sent a heartbeat, the response will likewise tell an existing member that a rebalance is under way.
* An existing member of the group can initiate a rebalance in the same way -- by sending a `JOIN_GROUP` request.

The `HEARTBEAT` responses to the members use the `REBALANCE_IN_PROGRESS` error code to indicate a rebalance.
Those clients have to rejoin the group within the rebalance timeout (measured from the point at which the failed client's session timed out).

In this way a new generation of the group is started. 

### Syncing the group
On receipt of the `JOIN_GROUP` response all the non-leaders will send `SYNC_GROUP` requests to the coordinator.

At this point the leader knows about the other members of the group and their subscriptions. 
Using this information is decides on each members assignment using the group state protocol.
It sends the assignments to the coordinator using a `SYNC_GROUP` request.
The coordinator then sends the non-leaders their assignments in the respective `SYNC_GROUP` responses.

### Offset commit

In order to avoid commits from zombies, <<OffsetCommit>> requests include the group generation id.
The coordinator receives the <<OffsetCommit>> request and validates the generation id.
If the generation is not current then the offsets are not committed.

### State machine

The coordinator maintains a state machine with the following states:

`PreparingRebalance`:: The group is preparing to rebalance.
  Transitions to:
  * `CompletingRebalance` if some members join before the timeout.
  * `Empty` if all members have left the group
  * `Dead` if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>).

`CompletingRebalance`:: The group is awaiting for state assignment (<<SyncGroup>>) from the group leader.
  Transitions to:
  * `Stable` if the coordinator receives a <<SyncGroup>> from the group leader.
  * `PreparingRebalance` if the coordinator receives a <<JoinGroup>> from a new member, or an existing member with new metadata; or if the coordinator receives a <<LeaveGroup>> from an existing member; or if member failure is detected (heartbeat timeout)
  * `Dead` if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>).

`Stable`:: Transitions to:
  * `PreparingRebalance` if the coordinator receives a <<JoinGroup>> from a new member, or an existing member with new metadata; or if the coordinator receives a <<LeaveGroup>> from an existing member; or if member failure is detected (heartbeat timeout)

`Dead`:: 
  There are no active members; the group metadata is being cleaned up.
  

`Empty`::
  The group has no members. The group continues in this state until offsets have expired. Transitions to:
  * `Dead` last offsets removed; or if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>); or if the group is removed by expiration.
  * `PreparingRebalnce` if a new member joins the group. 


.The group `GroupState` state Machine
[smcat]
....
PreparingRebalance,
CompletingRebalance,
Stable,
Dead,
Empty;

Stable => PreparingRebalance : "multiple reasons";
CompletingRebalance => PreparingRebalance : "leave group from existing member, or member failure detected";
Empty => PreparingRebalance;

PreparingRebalance => CompletingRebalance : "members joined in time";
CompletingRebalance => Stable : "sync group with assignment received";

Stable => Dead : "emigration";
PreparingRebalance => Dead : "emigration";
CompletingRebalance => Dead : " emigration";
Empty => Dead : "offset expiration";
Dead => Dead;

PreparingRebalance => Empty : "all members left";
....

### Coordinator crash

The coordinator stores the group generation, leader etc in the group topic (e.g. `__consumer_offsets` for consumer groups). 
If the coordinator fails the new leader of the group topic can read these state from its replica and carry on performing the coordinator role.
In effect the coordinator has migrated from the crashed broker to a live broker.

The clients detect failure of the coordinator because they don't receive a <<Heartbeat>> response within the session timeout, so they will find the new coordinator and start sending their heartbeats to it.
There is no need for the clients to perform the group membership or state protocols.

// TODO define what records are written to __consumer_state

## Group state protocol

The previous section described the group membership protocol, which allows a group to be established and maintained.

This section now goes into how the group state protocol, embedded within the `JOIN_GROUP` and `SYNC_GROUP` APIs, allows members of the group to partition "work" between them. The group state protocol is sometimes known as the "embedded protocol".

What constitutes work depends on the application:

* For the consumer it is the partitions to be consumed by the members of the group.

* For Kafka Connect it is the tasks for all the defined connectors.

In either case the `JOIN_GROUP` request from each member will contain a `Subscription`, which represents some work that the member wants done.
The `Subscription` from each member may be the same or different to other members.
The group leader receives the Subscriptions from all the members in the group once the group is established. 
The leader gets to divide the subscriptions between the members of the group and returns an `Assignment` to each member. 
An `Assignment` represents a members share of the work to be done. 

The separation of membership and state is possible because only the group members need to understand the group state.
The group state is opaque to coordinators and therefore:

* Coordinators cannot validate that completed work be being marked as done by the member which was assigned it (e.g. offsets are being committed). Instead clients are required to be bug-free in this respect to avoid work being done again following a rebalance.
* Coordinators cannot invalidate assignments when the topic's metadata changes, pushing this problem onto the client.


How a group works in a client (Consumer, Kafka Connect, Kafka Streams) depends on the  `AbstractCoordinator` implementation, which is responsible for generating all the group protocol requests, and handling all the responses, used by the client. That is, it:

* generates the  <<JoinGroup>> requests (including the necessary use data for each protocol the client can use)
* generates the <<SyncGroup>> request where the leader decides on the group state.

The Kafka Consumer and Kafka Connect use separate `AbstractCoordinator` implementations,`ConsumerCoordinator` and `WorkerCoordinator` respectively, because their group state differs:

* For the consumer it is a partitioning of the subscribed topic partitions
* For a Connect worker it is a partitioning of the tasks created by the connectors

`AbstractCoordinator.metadata()` supplies the _protocols_ which get send in the <<JoinGroup>> request.

// TODO elaborate on these classes a little.

For the `ConsumerCoordinator` there's another level of pluggability via the `ConsumerPartitionAssignor`. It's at the `ConsumerPartitionAssignor` level that Kafka Streams differs from the plain Kafka Consumer.
For the `ConsumerCoordinator` it's the assignors which correspond to the protocols in the <<JoinGroup>> request.

https://www.youtube.com/watch?v=QaeXDh12EhE

https://cwiki.apache.org/confluence/display/KAFKA/Incremental+Cooperative+Rebalancing%3A+Support+and+Policies#IncrementalCooperativeRebalancing:SupportandPolicies-Motivation

https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol


https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread

We'll now cover the assignors available to the Kafka Consumer. 
Later sections will cover the group state implemented by Kafka Connect and how Kafka Streams uses its own assignor.

### Range assignor
This is the simplest assignor. 
It does about the simplest possible assignment of partitions to consumers: 

* The _P_ partitions are ordered by topic name and partition id 
* The _C_ consumers are ordered by `member.id`
* Each consumer gets a batch _P/C_ or _P/C + 1_of partitions (when the division has a remainder)

For example, with partitions

* topicA-0
* topicA-1
* topicA-2
* topicB-0
* topicB-1

and consumers `C0` and `C1` we get:

[source]
.Range-assigned partitions
----
C0: topicA-0, topicA-1, topicA-2
C0: topicB-0, topicB-1
----

While this is _simple_ it's suffers from a number of practical problems:

* The work involved in consuming the partitions is unlikely to be evenly distributed. In particular, when consuming multiple topics this assignor gives partitions of the hottest topic to the same consumers.


### Round robin assignor
This is another simple assignor. 
It works a bit like the range assignor except is uses round robin allocation to consumers rather than allocating batches

For example, with partitions

* topicA-0
* topicA-1
* topicA-2
* topicB-0
* topicB-1

and consumers `C0` and `C1` we get:

[source]
.Round-robin-assigned partitions
----
C0: topicA-0, topicA-2, topicB-2
C0: topicA-1, topicB-1
----

This improves over the range assignor by more evenly distributing the partitions for hot topics.

When distributing the partitions, the algorithm will skip a consumer if its subscription didn't include that partition. 
This can result in imbalanced assignments. 

### Sticky assignor
KIP-54 added support for the Sticky assignor with the aim of improving on the round robin assignor in two respects:

* Guarantee a balanced assignment of partitions to consumers
* Try to avoid moving partitions between consumers during a rebalance (thus making use of buffered data already fetched by a consumer, but not yet consumed)

The constraint on balance overrides the constraint on stickiness.


### Cooperative sticky assignor
Assignment revocation provides a chance for the member to commit the work it has done. 
To avoid performing duplicate work the coordinator needs to ensure that a partition is revoked before it is reassigned.
The obvious way to guarantee this for all assignments to be revoked when a member joins a group, so that once the coordinator make its assignments all the work is committed and the newly-reassigned work can be shared via the `SYNC_GROUP` API.
This is known as a "stop-the-world rebalance".
While it's simple it means that all work stops for a period of time, with a consequent hit on throughput.
This doesn't really make sense for using sticky assignment, as often a member will get reassigned (substantially or exactly) the same partitions it was assigned before the rebalance. 

KIP-415 (Kafka Connect) and KIP-429 (for the Consumer) introduced cooperative sticky assignment, which allows members to carry on working during a rebalance, at the cost of needing multiple rebalances to reallocate work. 
Despite the need for multiple rebalances, this results in an overall improvement in throughput.

// TODO a load more detail needed

### Static membership protocol
KIP-345
// TODO

## Consumer architecture: `AbstractCoordinator`

.The group `MemberState` state Machine
[smcat]
....
Unjoined,
PreparingRebalance,
CompletingRebalance,
Stable;

Unjoined => PreparingRebalance  : "sent JoinGroup request, but no response yet" ;
PreparingRebalance => CompletingRebalance : "received JoinGroup response, but no assignment yet" ;
CompletingRebalance => Stable : "Joined; sending heartbeats" ;
....



## Answers to the motivating questions

What fetch-related state resides on a partition leader?::
For a non-incremental fetch nothing.
For an incremental fetch, the partitions of interest, but not the last-fetched offset.
When might a consumer successfully fetch from a partition follower?::
When the client is fetching from a follower in the same rack.
Which broker acts as group coordinator?::
The one that leading the relevant partition of `__consumer_offsets`.
How does a group detect and recover from the loss of (a) the coordinator, (b) a group member?::
The lack of any `HEARTBEAT` requests (for the coordinator), or responses (for the member) within the session timeout.
How many rebalances are required when a new member joins an established group, and how impactful is than on throughput?::
It depends on the coordinator/assignor.
Two for the cooperative stick assignor. Otherwise just one, but they'll be stop-the-world.