# The Producer

## Motivating questions

. What is the only Kafka RPC request that lacks a response?
. Can a zombie idempotent producer get fenced?
. What producer-related state resides on a partition leader?
. How are zombie producers prevented from writing duplicate records?
. Which broker acts as transaction coordinator?
. What is a control batch used for?


## Overview

Having done the necessary bootstrapping and discovered which brokers are acting as leader for the relevant partitions, a producer is finally in a position to send messages.
This is done using a `PRODUCE` request which essentially contains the messages to be appended to the log for some set of partitions led by that broker.

## The Simple producer

## `PRODUCE`

### `ProduceRequest`

The fields in the produce request should be fairly self-explanatory.

[id=ProduceRequest]
[source,javascript]
.The `PRODUCE` request
----
include::../kafka-sources/clients/src/main/resources/common/message/ProduceRequest.json[lines=16..-1]
----

### `ProduceResponse`

When the produce request has `acks=1` or `acks=all` the broker will respond with a `ProduceResponse` once all the records have been appended to its, or for `acks=all` enough follower's, logs.

NOTE: `acks=0` is a special case in the Kafka protocol: It's the only request which doesn't result in a response being returned.
That is, `acks=0` doesn't just mean the producer doesn't _wait_ for an acknowledgement, it means there is no acknowledgement at all.

[id=ProduceResponse]
[source,javascript]
.The `PRODUCE` response
----
include::../kafka-sources/clients/src/main/resources/common/message/ProduceResponse.json[lines=16..-1]
----

Of course there's the possibility that the broker is not the leader for some of the partitions in the request.
In this case the `NOT_LEADER_FOR_PARTITION` error will prompt the producer to refresh it's metadata and resend the records for those partitions to the new leader, if there is one. 
If there is no new leader the messages will eventually expire.

What can be inferred when a `PRODUCE` request is sent but no `PRODUCE` response is received? Nothing! 
The sender has no idea whether the request reached a broker or not, and must choose between resending the message (risking a duplicate message being appended to the log), or not resending (risking loss of the message). 

### The record format
// Record format

## Producer architecture

[source]
....
              +---------------+        +-------------+
    send()--> | KafkaProducer | -----> | Partitioner |
              +---------------+        +-------------+
                |         |
                V         +---------------------+ 
+-------------------+     |                     |
| RecordAccumulator |     |                     |
+-------------------+     |      Â               |
                ^         V                     V
                |    +--------+         +------------------+
                +--- | Sender | ------> | ProducerMetadata |
                     +--------+         +------------------+
                          |
                          V
                  +---------------+
                  | NetworkClient | --> to network
                  +---------------+
....

// This is a bit of a sketch, and misses out important details, like how METADATA requests actually get sent
// and transactions

When `send()` is called, on the application thread, the following happens:

. Any configured interceptors are run
. Fetch topic metadata if necessary (`ProducerMetadata`)
. The key and value are serialized
. The `Partitioner` assigns a partition
. The compressed size of the record is estimated
. The record may be added to the last batch in the `Accumulator`
. If the last batch is full an new batch is created
.. The `Partitioner` _reassigns_ the partition
.. `Accumulator`: Append to new batch
. Wake the `Sender` if the batch last batch is now full or a new batch was created

The `Sender` is running continually on the IO thread:

. Ask the accumulator for the brokers for which there are ready batches
. If there are any topics with an unknown leader, request metadata update
. Filter out the brokers where the network is not ready for sending requests
. Drain batches from the `Accumulator` for each ready broker
. Create `PRODUCE` requests and send them to the `NetworkClient`

The `RecordAccumulator` uses a deque:

* The application thread adds to the back of the deque
* The IO threads removes from the front of the deque
* Failed sends are pushed back on the front of the deque, so that message order is maintained.

## The idempotent producer

Support for idempotent producers was added in https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging[KIP-98].

A `ProduceRequest` is not, on its own, idempotent.
In particular, if the client doesn't receive a response it may resend a produce request, resulting in duplicate records in the log. 
It is made idempotent by adding a sequence number to each record prior to sending it the first time and having the broker keep track of the sequence number of the last appended record in each partition. 
If the broker observes a new record batch whose sequence number is less than or equal to this last appended record it doesn't append the record, but acknowledges it back to the client anyway (since that records must already be in the log).
Likewise if the broker observes a batch whose sequence number is larger than the next expected number is will also be rejected, because there must be another batch, as-yet unobserved, which needs to be appended _first_.
// Should we mention the producer epoch here?

The observed sequence number needs to be per-producer and its lifetime is that of the producer process (not its connection to the broker). 
The broker doesn't otherwise know when a producer process or session ends, so there needs to be an explicit way for the broker to identify produce requests from the same producer session.
This is the purpose of the producer id.

### Obtaining an idempotent Producer Id (PID) (`INIT_PRODUCER_ID`)

The producer asks for an identifier from any broker and sends this in each record along with its sequence number for that partition.
The sequence is basically tied to that PID, so if the producer restarts it will request a PID, be granted a new one and start the sequence again from zero.
// Is it zero? Yes, but the producer epoch will be increased so this doesn't cause issues with lower sequence number.

#### `InitProducerIdRequest`

The exact behaviour depends on whether the producer is transactional (i.e. has `transactional.id` set), or merely idempotent (i.e. does not have `transactional.id` set)

[id=InitProducerIdRequest]
[source,javascript]
.The `INIT_PRODUCER_ID` request
----
include::../kafka-sources/clients/src/main/resources/common/message/InitProducerIdRequest.json[lines=16..-1]
----

#### `InitProducerIdResponse`

[id=InitProducerIdResponse]
[source,javascript]
.The `INIT_PRODUCER_ID` response
----
include::../kafka-sources/clients/src/main/resources/common/message/InitProducerIdResponse.json[lines=16..-1]
----

What can be inferred when an idempotent `PRODUCE` request is sent but no `PRODUCE` response is received? Still nothing! 
But crucially the decision about whether to resend is easy: Resending doesn't risk duplicate messages being appended to the log.

## The transactional producer

Support for transactions was also added in https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging[KIP-98].

This allows a producer to send messages to a set of partitions (with leaders on multiple brokers) within a transaction that is committed or aborted atomically. It also guarantees atomicity when sending multiple produce requests to the same broker.
"Atomic" means either all the messages in the transaction become visible to (suitably configured) consumers (if the transaction was committed) or none of them do (if the transaction was aborted).
A transactional producer is also idempotent.

Because logs are immutable and records get appended during the transaction and not at the end, it is necessary for transactional consumers (i.e. those with an isolation level of `read_committed`) to buffer incomplete transactions in memory. Special control records, called "end transaction markers" or "commit markers", are present in the log to mark the end of a transaction. If the marker shows the transaction was aborted the buffered records are silently dropped by the client library and not passed on to the application.

The producer uses a number of additional requests to achieve this. 
Extra interbroker communication is also required.

1. The producer must find the broker which is acting as its transaction coordinator.

2. The producer gets a _transactional_ producer id from its coordinator. Unlike a PID for a purely idempotent producer, a transactional PID identifies the producer across multiple sessions.

3. The producer sends produce requests to partition leaders (typically other brokers than the coordinator), and tells its coordinator which partitions and offsets it is interacting with.

4. The producer send and end transaction message to its coordinator, either committing it, or rolling it back.

The producer than then repeat steps 3 to 4 indefinitely, or until the producer becomes fenced. 

[[coordinator-discovery,coordinator discovery]]
### Coordinator discovery (`FIND_COORDINATOR`)

NOTE: Coordinator discover is used for both transactional producers and consumer groups, so in this section we'll talk about the client, rather than the producer. We'll refer back to this section later then discussing consumer groups.

Coordinator discovery is about unambiguously identifying a unique broker based on some identity. 
For the transactional producer that identity is its `transactional.id`. 
For a member of a consumer group that identity is its `group.id`.
Let's use the word "subjects" to refer to these coordinator-requiring entities in general. 

1. The subject queries a random broker, giving the required coordinator type and the subject's identity in a `FIND_COORDINATOR` request.
2. The random broker receives the request and determines the coordinator according to
+
[source]
----
// Pseudocode
partitionId = abs(hashCode(identity)) mod numPartitions(topicName)
coordinator = leaderOf(topicName, partitionId)
----
+
Where the `topicName` is `+__consumer_offsets+` for the consumer group coordinator and `+__transaction_state+` for the transaction coordinator. 
In other words, the leaders of partitions of those topics have an additional role on top of their leadership.
They have to manage extra broker-side state for the subjects they're coordinating.

3. The client receives a `FIND_COORDINATOR` response and starts talking to this coordinator.

#### `FindCoordinatorRequest`

[id=FindCoordinatorRequest]
[source,javascript]
.The `FIND_COORDINATOR` request
----
include::../kafka-sources/clients/src/main/resources/common/message/FindCoordinatorRequest.json[lines=16..-1]
----

#### `FindCoordinatorResponse`

[id=FindCoordinatorResponse]
[source,javascript]
.The `FIND_COORDINATOR` response
----
include::../kafka-sources/clients/src/main/resources/common/message/FindCoordinatorResponse.json[lines=16..-1]
----

### Obtaining an transactional Producer Id (PID)

This is basically the same `INIT_PRODUCER_ID` API we've seen for the idempotent producer, except:

* it must be send to the transaction coordinator,
* the request includes the `transactional.id` and the transaction timeout

The association between the transactional id and its PID is written to the relevant `+__transaction_state+` partition by the coordinator.
This allows the same PID to be issued to the same (as identified by transactional id) producer even in the event the leader changes (e.g. due to a broker crash) or the producer process restarts.

These persistent PIDs come with an epoch for fencing zombie producers. 
For example if a new instance of a producer is started, but somehow the old one is still running, the request for a PID from the new instance will increment the producer epoch so that RPC from the old one are ignored.

This means that the choice of `transactional.id` is crucial if zombie producers are going to be effectively fenced off.
It needs to identify the "logical producer", and the partitions it will be producing to.
It needs to be stable across restarts of that producer process.
If a pool of producers were being used, _something_ has to guarantee that individual messages can only be sent by one producer from the pool.
If it's possible, in the application logic, for an individual message to be sent multiple times by the producer instances (possibly as the result of some higher level retry strategy) then duplicates are possible and the transactional semantics are lost.

### Beginning a transaction

There is no explicit RPC for starting a transaction. 
The producer API method only changes local state.

### Adding partitions to a transaction (`ADD_PARTITIONS_TO_TXN`)

The first time the producer sends a `ProduceRequest` to a new partition (that's not yet part of this transaction) it will also send an `AddPartitionsToTxnRequest` to the coordinator.
The coordinator logs this state so that when the transaction is ended it knows which brokers need to be sent requests to write transaction markers.

#### `AddPartitionsToTxnRequest`

[id=AddPartitionsToTxnRequest]
[source,javascript]
.The `ADD_PARTITIONS_TO_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/AddPartitionsToTxnRequest.json[lines=16..-1]
----

When the coordinator receives such a request it will append to the log for the relevant `+__transaction_state+` partition,
recording the producer (PID) and the partition, before sending the response.

#### `AddPartitionsToTxnResponse`

[id=AddPartitionsToTxnResponse]
[source,javascript]
.The `ADD_PARTITIONS_TO_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/AddPartitionsToTxnResponse.json[lines=16..-1]
----

// Example transactional PRODUCE with PID, epoch, sequence number

### Adding offsets to a transaction (`ADD_OFFSETS_TO_TXN`)

Streaming applications (that is, those where records consumed from one set of partitions cause new records to be written the other partitions) need a way to add the consumer's offsets to the transaction, since such offset commit also results in appending records to the `+__consumer_offsets+` partition.
If `+__consumer_offsets+` was not included in the transaction (which means if it didn't get markers written on transaction completion) it would be possible for the downstream partitions to get out of sync with the upstream partitions.

#### `AddOffsetsToTxnRequestRequest`

The `AddOffsetsToTxnRequest` is thus the `+__consumer_offsets+` flavour of the `AddPartitionsToTxnRequest` for a normal topic, and is sent to the _transaction_ coordinator.

[id=AddOffsetsToTxnRequestRequest]
[source,javascript]
.The `ADD_OFFSETS_TO_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/AddOffsetsToTxnRequest.json[lines=16..-1]
----

As for the `AddPartitionsToTxnRequest`, the coordinator will append a record to the `+__transaction_state+` partition recording that the producer interacted with `+__consumer_offsets+`, before sending the response.

#### `AddOffsetsToTxnRequestResponse`

[id=AddOffsetsToTxnRequestResponse]
[source,javascript]
.The `ADD_OFFSETS_TO_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/AddOffsetsToTxnResponse.json[lines=16..-1]
----

#### `TxnOffsetCommitRequest`

The `TxnOffsetCommitRequest` is the `+__consumer_offsets+` flavour of the `ProduceRequest` for a normal topic and is send to the _group_ coordinator.

[id=TxnOffsetCommitRequest]
[source,javascript]
.The `TXN_OFFSET_COMMIT` request
----
include::../kafka-sources/clients/src/main/resources/common/message/TxnOffsetCommitRequest.json[lines=16..-1]
----

#### `TxnOffsetCommitResponse`

[id=TxnOffsetCommitResponse]
[source,javascript]
.The `TXN_OFFSET_COMMIT` response
----
include::../kafka-sources/clients/src/main/resources/common/message/TxnOffsetCommitResponse.json[lines=16..-1]
----

Transactional producers which are not also consumers don't need to use these RPCs, which are driven by use of the `sendOffsets()` API on the client.

### Ending a transaction (`END_TXN`, `WRITE_TXN_MARKERS`)

The producer ends the transaction when the application calls `commitTransaction()` or `abortTransaction()`.
In either case the producer sends a `EndTxnRequest` to the transaction coordinator. 

#### `EndTxnRequest`

[id=EndTxnRequest]
[source,javascript]
.The `END_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/EndTxnRequest.json[lines=16..-1]
----

The coordinator writes another record to `+__transaction_state+` recording that the producers transaction was ended.
It can then send a response back to the client.

#### `EndTxnResponse`

[id=EndTxnResponse]
[source,javascript]
.The `END_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/EndTxnResponse.json[lines=16..-1]
----


#### `WriteTxnMarkersRequest`
Although from the producer's perspective the transaction is now complete, the transaction coordinator's work is not yet done.
The coordinator uses the accumulated state for the transaction in `+__transaction_state+` to complete the transaction. 
It does this by sending `WriteTxnMarkersRequests` to each of the brokers leading the partitions added to the transaction, including the `+__consumer_offsets+` partitions if `sendOffsets()` was used. The transaction markers record whether the transaction was aborted or committed, and will be used later to filter out aborted transactions on consumers with the `read_committed` isolation level.

[id=WriteTxnMarkersRequest]
[source,javascript]
.The `WRITE_TXN_MARKERS` request
----
include::../kafka-sources/clients/src/main/resources/common/message/WriteTxnMarkersRequest.json[lines=16..-1]
----

#### `WriteTxnMarkersResponse`

[id=WriteTxnMarkersResponse]
[source,javascript]
.The `WRITE_TXN_MARKERS` response
----
include::../kafka-sources/clients/src/main/resources/common/message/WriteTxnMarkersResponse.json[lines=16..-1]
----

#### HA of the coordinator

Note that by writing to the `+__transaction_state+` log at every step, the coordinator is able to recover from a broker crash.
If this happens, the leadership of the `+__transaction_state+` partition will move to a replica, and so the new leader will also become the new coordinator.
The new coordinator has access to the transaction state since it was already replicated via the normal log replication protocol.
In this way the new coordinator is able to pick up where the old coordinator left off. 

// TODO transaction timeout

### The `TransactionManager`

On the producer, transactions are managed by the `TransactionManager`. 
This was left out of the previous diagram

[source]
....
                +---------------+        +-------------+
     send()---> | KafkaProducer | -----> | Partitioner |
                +---------------+        +-------------+
                 |          |  |
                 |          |  +----------------+ 
                 V          V                   |
+-------------------+     +------------+        |
| RecordAccumulator | --> | TxnManager |        |
+-------------------+     +------------+        |
                ^            ^                  |
                |            |                  V
                |    +--------+         +------------------+
                +--- | Sender | ------> | ProducerMetadata |
                     +--------+         +------------------+
                          |
                          V
                  +---------------+
                  | NetworkClient | --> to network
                  +---------------+
....

The transaction manager uses a state machine to determine what transactional requests need to be sent.

.The `TransactionManager` State Machine
[smcat]
....
"Uninit",
Initializing,
Ready,
InTransaction,
CommittingTxn,
AbortingTxn,
AbortableError,
FatalError;

Ready => "Uninit";
"Uninit" => Initializing;
AbortingTxn => Initializing;
Initializing => Ready;
CommittingTxn => Ready;
AbortingTxn => Ready;
Ready => InTransaction;
InTransaction => CommittingTxn;
InTransaction => AbortingTxn;
AbortableError => AbortingTxn;
InTransaction => AbortableError;
CommittingTxn => AbortableError;
AbortableError => AbortableError;
"Uninit" => FatalError;
Initializing => FatalError;
Ready => FatalError;
InTransaction => FatalError;
CommittingTxn => FatalError;
AbortingTxn => FatalError;
AbortableError => FatalError;
....

// TODO write some more about the TransactionManager


## Answers to the motivating questions

. What is the only Kafka RPC request that lacks a response? `PRODUCE` with `acks=0`
. Can a zombie idempotent producer get fenced? No, because fencing is based on the `transactional.id`, which isn't set for a purely idempotent producer
. What producer-related state resides on a partition leader? For an idempotent or transactional producer the producer epoch and sequence number.
. How are zombie producers prevented from writing duplicate records? The coordinator will increment the epoch for the producer; on observing a higher epoch, leaders will fence out lower epochs.
. Which broker acts as transaction coordinator? The leader of of the relevant partition of `+__transaction_state+`
. What is a control batch used for? Marking whether a transaction was committed or aborted.
