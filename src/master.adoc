// asciidoctor -r asciidoctor-diagram master .adoc

# Kafka internals
Tom Bentley
:toc: left  
:toclevels: 4
:source-highlighter: pygments
:icons: font


:leveloffset: +1

This document is about the _implementation_ of Apache Kafka.
It does not address why you might want to use Kafka, or how to do anything in particular with it.
Instead it discusses both the protocol and the code in depth. 
It is largely aimed as answering the question "How does Kafka work?".
It assumes you have some familiarity with Kafka concepts such as topics, partitions, replicas, logs and segments.

The content is broken into three parts. 
First the Kafka protocol is introduced, mostly without talking about implementation details.
Second we talk about the implementation of the Apache Kafka clients, broker and other things.
Finally we try to put this theoretical knowledge to use by talking about how it relates to real problems.

# The Kafka Protocol

In this part we cover most of the Kafka protocol in the abstract.
We're not concerned here with how messages get serialized, networking, threading or any other implementation concerns.
This is purely about what information flows between which peers, and what they might do with it.


:leveloffset: +1

include::protocol-overview.adoc[]

include::bootstrap.adoc[]

include::producer.adoc[]

include::consumer.adoc[]

:leveloffset: -1

## Broker metadata

// LEADER_AND_ISR, UPDATE_METADATA, METADATA_FETCH

The controller broadcasts changes to metadata to other brokers via the
<<UpdateMetadata>> request.

When a broker receives an <<UpdateMetadata>> request it:

* updates its metadata cache, 
* complete any operations which were in purgatory waiting for some change which was present in the new metadata,
* sends a response back to the controller.

When the controller receives an <<UpdateMetadata>> response it basically does nothing (only logs). Thus the logic of the controller does not depend on knowing that a metadata request was received.

ASIDE: I suspect this is because, if a request was not delivered to a broker (e.g. due to crash) then, on restart, the broker will get fresh metadata anyway.

**TODO** How and when does the controller send updatametadata requests

**TODO** Discuss difference between clients and brokers wrt metadata propagation.

:leveloffset: +1

include::log_append.adoc[]

:leveloffset: -1

// Replication protocol
include::replication_protocol.adoc[]

## Reassignment

## Kraft

## Admin requests

// LIST_GROUPS, DESCRIBE_GROUPS
// *_ACLS
// CREATE_/DELETE_TOPICS, CREATE_PARTITIONS
// DELETE_RECORDS
// Other DESCRIBE_*
// KIP-700 DESCRIBE_CLUSTER

# Implementation

//
// Part 2: Implementation

//


//
## Broker
### Startup
### Shutdown
#### Controller shutdown
#### Full disks
#### Crash
### ReplicaManager
### RSM
### GroupCoordinator



### TxnCoordinator
### GroupCoordinator
//
##  Controller
//
## Kafka Connect
//
## Kafka Streams
//
// Part 3: Operations
//
// Appendices
//   KIP index
//   RPC index
//   Index of message schemas (both public and internal)

// Very old content which should be rewritten
//
// include::authentication.adoc[]

// include::producer.adoc[]

// include::consumer.adoc[]
