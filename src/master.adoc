// asciidoctor -r asciidoctor-diagram master .adoc

# Kafka internals
Tom Bentley
:toc: left  
:toclevels: 4
:source-highlighter: pygments
:icons: font


:leveloffset: +1

This document is about the _implementation_ of Apache Kafka.
It does not address why you might want to use Kafka, or how to do anything in particular with it.
Instead it discusses both the protocol and the code in depth. 
It is largely aimed as answering the question "How does Kafka work?".
It assumes you have some familiarity with Kafka concepts such as topics, partitions, replicas, logs and segments.

The content is broken into three parts. 
First the Kafka protocol is introduced, mostly without talking about implementation details.
Second we talk about the implementation of the Apache Kafka clients, broker and other things.
Finally we try to put this theoretical knowledge to use by talking about how it relates to real problems.


# The Kafka Protocol

In this part we cover most of the Kafka protocol in the abstract.
We're not concerned here with how messages get serialized, networking, threading or any other implementation concerns.
This is purely about what information flows between which peers, and what they might do with it.

## APIs

Kafka's is almost uniformly a request-response protocol that runs on TCP or TLS transports. 
Requests and responses are sometimes collectively called "messages", or RPCs (remote procedure calls). "Messages" is a little confusing since records are also often called "messages", so we'll use the term RPC instead.

// define API key
Each request includes a number, called the API key, which identifies the type of request. For example, the produce request, used to send records to a broker, has API key 0, and the fetch request has the API key 1.

RPCs consist of a header and a body. 

### Request header
For requests the header looks like this:

[id=RequestHeader]
[source,javascript]
.The request header
----
include::../kafka-sources/clients/src/main/resources/common/message/RequestHeader.json[lines=16..-1]
----

We'll discuss the `RequestApiVersion` shortly. 
The `CorrelationId` allows brokers to respond to requests in a different order than the client sent them in: 
The client uses a unique id for each request, which is included in the response, and the client matches the response to the request using the id.

### Response header
Responses have their own header, which just includes the correlation id.

[id=ResponseHeader]
[source,javascript]
.The response header
----
include::../kafka-sources/clients/src/main/resources/common/message/ResponseHeader.json[lines=16..-1]
----

Although it's not part of the response header, must responses have a top-level field in the body called `ThrottleTimeMs`, which we'll discuss here.
Brokers need to be able to throttle clients but they can't control how quickly clients send requests.
Therefore the only mechanism the broker can use to throttle clients is to not return a response immediately when the client's quota is exceeded.
This would be completely transparent to the client, so the `ThrottleTimeMs` field is used to inform the client that the response was delayed for the given amount of time.
A sufficiently sophisticated client could use this to slow the rate it sends RPCs to that broker.
This would make most sense for those requests which can be sent to any broker.


## Participants
// define client, broker, producer, consumer, follower, leader, controller, zk

Let's define the participants in the Kafka protocol.

### Clients

The applications which talk to a cluster from the outside are _clients_. These include _producers_, which send messages to the cluster to be appended to logs, and _consumers_ which read from those logs.
It also includes things like the _admin_ client, which is used to manage the cluster.
Although Apache Kafka packages its clients as three separate classes this distinction is not meaningful at the protocol level. 
What defines a client is its use of the client-facing APIs.

### Brokers

Clients usually talk to _brokers_, which are network server processes with the main purpose of storing logs. 
When talking about a broker in relation to a particular partition it may be acting in the _leader_ or _follower_ role. In other words, a _leader_ or _follower_ is a broker acting in a certain role with respect to a given partition.

### Controller

The _controller_ is a network server process with the task of coordinating the metadata necessary for the operation of the cluster. 
In ZK-mode the controller is a broker which takes on the controller role (for a time).
In KRaft mode the controller is _usually_ one of a separate set of network servers making up the raft cluster. 
In non-production deployments the raft cluster may instead be made up of a subset of the brokers.

### Kraft nodes

### ZooKeeper nodes

### Servers

This document will use the term "server" to collectively refer to something that could be a broker, a KRaft node or possibly a zookeeper node and a more specific term would be incorrect.
For example, both brokers and kraft nodes support the METADATA RPC, so "server" would be the appropriate term for the recipient of a metadata request.

## Client bootstrapping
// API_VERSIONS, SASL*, METADATA

When a client starts up it has no existing knowledge of the cluster state, it only has the set of "bootstrap servers" with which it has been configured.
Bootstrapping is the process of connecting to one of those servers and discovering the necessary cluster state in order to perform whatever that client is supposed to do. 
The necessary cluster state includes the available brokers and their addresses and possibly information about topics of interest.

In order to do this it needs:

1. To know how to talk to the bootstrap broker it connects to. 
The Kafka RPCs are individually versioned, so in order to send requests that the server will understand, the Apache Kafka clients need to know the versions supported by the server.
The `API_VERSIONS` request allows a client to discover this.
By sending a `API_VERSIONS` request first, a client can determine which versions of all the other RPCs it should use by taking `max(intersection(supported_client_versions, supported_broker_versions))` for that particular API.

2. Possibly to authenticate to the server. The `API_VERSIONS` request is the only one which a listener requiring authentication will handle without requiring authentication of its peer. 
If TLS is used for authentication then the server will already know the client's identity as a result of the TLS handshake. 
If SASL is being used then `SASL_HANDSHAKE` and `SASL_AUTHENTICATE` are exchanged.

3. To discover information about the cluster, including the currently live brokers and information about certain topics of interest to the client.
This is done using `METADATA` requests.
One response with the information about the live brokers is enough to bootstrap the client, but clients typically make more later as the state of the client and cluster changes. 

The process described above is common to all _Apache_ Kafka clients.
Other client libraries sometimes work slightly differently.
For example, a Sarama client has to be configured with the version of the broker it is talking to and uses a hard-coded mapping from "broker version" to supported API versions. 

In the following sections we'll explore these requests in detail.

### `API_VERSIONS`

#### `ApiVersionsRequest`

The API_VERSIONS request is trivial, having only a couple of optional fields for telling the server the name and version of the client library.
// TODO for each request: required authz, idempotency
[id=ApiVersionsRequest]
[source,javascript]
.The API_VERSIONS request
----
include::../kafka-sources/clients/src/main/resources/common/message/ApiVersionsRequest.json[lines=16..-1]
----

#### `ApiVersionsResponse`

The response is essentially composed of a object for each API/RPC key, containing the maximum and minimum version supported by the broker. 
[id=ApiVersionsResponse]
[source,javascript]
.The API_VERSIONS response
----
include::../kafka-sources/clients/src/main/resources/common/message/ApiVersionsResponse.json[lines=16..-1]
----

// client doesn't know broker version
NOTE:: The API versions response does not include a server software version, e.g. that a broker is running Kafka 3.0. 
This is intentional as it discourages clients from becoming dependent on the broker version (e.g. using it to work around bugs).
The rationale is that it's better that the broker be fixed by a software update than for clients implement version-specific behaviour.
Of course this does not prevent the set of RPCs and their maximum and minimum versions being used to infer the broker version.

// Feature gates
KIP-584 added support for 'feature gates', which is a mechanism for versioning features which apply at the cluster level (rather than per-broker API versioning).
The supported and enabled features are also included.
Features are only supported when all servers in a cluster support them, and are enabled by an administrator.

### SASL authentication (`SASL_HANDSHAKE` and `SASL_AUTHENTICATE`)

KIP-12 added support for SASL/GSSAPI (aka Kerberos v5) and SSL/TLS. These are both negotiated in the transport layer, and so there's no need for application-level APIs for them.

#### `SaslHandshakeRequest` 

The request nominates a SASL mechanism

[id=SaslHandshakeRequest]
[source,javascript]
.The `SASL_HANDSHAKE` request
----
include::../kafka-sources/clients/src/main/resources/common/message/SaslHandshakeRequest.json[lines=16..-1]
----

#### `SaslHandshakeResponse` 

The response either rejects the request (`UNSUPPORTED_SASL_MECHANISM`) with a list of the supported mechanisms, or agrees to the selected mechanism.

[id=SaslHandshakeResponse]
[source,javascript]
.The `SASL_HANDSHAKE` response
----
include::../kafka-sources/clients/src/main/resources/common/message/SaslHandshakeResponse.json[lines=16..-1]
----

What happens next depends on the support broker and client versions.

For `SASL_HANDSHAKE` version 0 (defined in KIP-43), on receipt of the response the client stops sending normal Kafka-framed RPCs and starts a SASL exchange over the connection, serialized as a simple Kafka array without any message header.
Once this exchange has completed (and resulted in success) Kafka RPCs can again be sent.

[source]
.Example v0 SASL/Plain conversation (adapted from RFC-4616) showing framing/encapsulation
----
C(Kafka): SaslHandShake(mechanism=plain)  <1>
S(Kafka): SaslHandshake(error=0)          <2>
C(SASL): <NUL>tim<NUL>tanstaaftanstaaf    <3>
S(SASL): // return an empty SASL frame    <4>
... further Kafka frames
----
<1> I want to use Plain
<2> OK, use plain
<3> I'm tim, my password is tanstaaftanstaaf
<4> I agree, you are

While this works for SASL/Plain there is no mechanism-independent way for a server to tell a client the exchange has failed.
This makes it hard for clients to handle failed exchanges.

#### `SaslAuthenticateRequest` 

For version `SASL_HANDSHAKE` 1+ (defined by KIP-152) the SASL exchange is wrapped in Kafka-level `SASL_AUTHENTICATE` requests and responses

[source]
.Example v1 SASL/Plain conversation (adapted from RFC-4616) showing Kafka framing
----
C(Kafka): SaslHandshake(mechanism=plain)
S(Kafka): SaslHandshake(error=0)
C(Kafka): SaslAuthenticate(sasl_auth_bytes=<NUL>tim<NUL>tanstaaftanstaaf) <1>
S(Kafka): SaslAuthenticate(error_code=0, sasl_auth_bytes=) // empty       <2>
... further Kafka frames
----
<1> Encapsulated in a Kafka request
<2> Encapsulated in a Kafka response

[id=SaslAuthenticateRequest]
[source,javascript]
.The `SASL_AUTHENTICATE` request
----
include::../kafka-sources/clients/src/main/resources/common/message/SaslAuthenticateRequest.json[lines=16..-1]
----

#### `SaslAuthenticateResponse` 

[id=SaslAuthenticateResponse]
[source,javascript]
.The `SASL_AUTHENTICATE` response
----
include::../kafka-sources/clients/src/main/resources/common/message/SaslAuthenticateResponse.json[lines=16..-1]
----

Support for SASL/SCRAM (KIP-84) and SASL/OAUTHBEARER (KIP-255) works in the same way, though SCRAM requires multiple `SASL_AUTHENTICATE` exchanges.


// TODO example logging of successful and failed SASL authentication

### Metadata requests

// The initial cluster request
The minimum information needed to bootstrap a client is the set of live brokers and how they can be connected to.
The client obtains this using a `METADATA` request to a bootstrap broker (having first found out its supported API versions and authenticated to it).

#### `MetadataRequest`

[id=MetadataRequest]
[source,javascript]
.The `METADATA` request
----
include::../kafka-sources/clients/src/main/resources/common/message/MetadataRequest.json[lines=16..-1]
----

// TODO example log for a bootstrap metadata request (explain the negative broker ids)

// Topic requests
Information about some (possibly empty) set of topics can be requested at the same time. 
// Topic creation by side-effect
The possibility of creating topics as a side-effect means that metadata requests are not idempotent, and is not good protocol design.
Support for this was added before the Admin client was a viable alternative for creating topics explicitly.
// authorized operations
Support for obtaining information about what the authenticated principal can do was also an ad-doc addition to the API to support the Admin client. 

// Refreshing metadata
The Apache Kafka clients have a cache of the metadata and will refresh it when:

* they observe a response which an error code which implies that the metadata is out of date
* periodically, if it's not otherwise been refreshed

The producer and consumer metadata handling is entirely managed in this way and hence is hidden from the user. 
In the admin client, the user visible methods such as `describeCluster()`, `listTopics()` and `describeTopics()` will always send a metadata request, rather than serving possibly stale data from the cache. 
The cache itself is not updated by these explicit requests either: The two paths for metadata requests are completely independent. 

#### `MetadataResponse`

[id=MetadataResponse]
[source,javascript]
.The `METADATA` response
----
include::../kafka-sources/clients/src/main/resources/common/message/MetadataResponse.json[lines=16..-1]
----

The cluster is described by the _live_ brokers in the `Brokers` array.
No information about non-live brokers is included, even though their existence may be inferred from other parts of the response (e.g. `Topics/Partitions/ReplicaNodes`.

When a partition doesn't have a leader its `LeaderId` will be -1.

// TODO example logging for a the bootstrap metadata response

// TODO Metadata in the admin client (CLUSTER_METADATA)

### Summary

A "typical" client bootstrapping exchange looks like this:
[source]
.Typical successful client bootstrapping
----
C: ApiVersions v3
S: ApiVersions v3 (ErrorCode=0,
                   ApiKeys=[{ApiKey=SaslHandshake, Min=0, Max=1},
                            {ApiKey=SaslAuthenticate, Min=0, Max=2},
                            {ApiKey=Metadata, Min=0, Max=11}, ...])
C: SaslHandshake v1 (Mechanism=plain)
S: SaslHandshake v1 (ErrorCode=0)
C: SaslAuthenticate v2 (sasl_auth_bytes=<NUL>tim<NUL>tanstaaftanstaaf)
S: SaslAuthenticate v2 (ErrorCode=0, sasl_auth_bytes=) // empty
C: Metadata v11 ()
S: Metadata v11 (ErrorCode=0, Brokers=...)
----

And whenever a client is disconnected from a broker we'd expect to see the same pattern, minus the metadata request/response.

## The simple Producer

Having done the necessary bootstrapping and discovered which brokers are acting as leader for the relevant partitions, a producer is finally in a position to send messages.
This is done using a `PRODUCE` request which essentially contains the messages to be appended to the log for some set of partitions led by that broker.

### `PRODUCE`

#### `ProduceRequest`

The fields in the produce request should be fairly self-explanatory.

[id=ProduceRequest]
[source,javascript]
.The `PRODUCE` request
----
include::../kafka-sources/clients/src/main/resources/common/message/ProduceRequest.json[lines=16..-1]
----

#### `ProduceResponse`

When the produce request has `acks=1` or `acks=all` the broker will response with a `ProduceResponse` once all the records have been appended to its, and for `acks=all` enough follower, logs.

NOTE: `acks=0` is a special case in the Kafka protocol: It's the only request which doesn't result in a response being returned.
That is, `acks=0` doesn't just mean the producer doesn't _wait_ for an acknowledgement, it means there is not acknowledgement at all.

[id=ProduceResponse]
[source,javascript]
.The `PRODUCE` response
----
include::../kafka-sources/clients/src/main/resources/common/message/ProduceResponse.json[lines=16..-1]
----

Of course there's the possibility that the broker is not the leader for some of the partitions in the request.
In this case the XXXX error will prompt the producer to refresh it's metadata and resend the records for those partitions to the new leader, if there is one. 
If there is no new leader the messages will eventually expire.

### The record format
// Record format

## The idempotent producer

Support for idempotent producers was added in KIP-98.

A `ProduceRequest` is not, on its own, idempotent.
In particular if the client doesn't receive a response it may resend a produce request, resulting in duplicate records in the log. 
It is made idempotent by adding a sequence number to each record prior to sending it the first time and having the broker keep track of the sequence number of the last appended record in each partition. 
If the broker observes a new record whose sequence number is <= this last appended record it doesn't append the record, but acknowledges it back to the client anyway (since that records must already be in the log).

The observed sequence number needs to be per producer and its lifetime is that of the producer process (not it's connection to the broker). 
The broker doesn't otherwise know when a producer process or session ends, so there needs to be an explicit way for the broker to identify produce requests from the same producer session.
This is the purpose of the producer id.

### Obtaining an idempotent Producer Id (PID) (`INIT_PRODUCER_ID`)

The producer asks for an identifier from any broker and sends this in each record along with its sequence number for that partition.

#### `InitProducerIdRequest`

The exact behaviour depends on whether the producer is transactional (i.e. has `transactional.id` set), or merely idempotent (i.e. does not have `transactional.id` set)

[id=InitProducerIdRequest]
[source,javascript]
.The `INIT_PRODUCER_ID` request
----
include::../kafka-sources/clients/src/main/resources/common/message/InitProducerIdRequest.json[lines=16..-1]
----

#### `InitProducerIdResponse`

[id=InitProducerIdResponse]
[source,javascript]
.The `INIT_PRODUCER_ID` response
----
include::../kafka-sources/clients/src/main/resources/common/message/InitProducerIdResponse.json[lines=16..-1]
----


## The transactional producer

Support for transactions was added in KIP-98.
This allows a producer to send messages to a set of partitions (with leaders on multiple brokers) within a transaction that is committed or aborted atomically.
"Atomically" means either all the messages in the transaction become visible to (suitably configured) consumers (if the transaction was committed) or none of them do (if the transaction was aborted).

Because logs are immutable and records get appended during the transaction it is necessary for transactional consumers (i.e. those with an isolation level of `read_committed`) to buffer incomplete transactions in memory. Special control records are present in the log to mark the end of a transaction. If the marker shows the transaction was aborted the buffered records are silently dropped and not passed on to the application.

To enable this, the producer uses a number of additional requests.

1. It must find the broker which is acting as its transaction coordinator.

2. It gets a _transactional_ producer id (which, unlike a pid for a purely idempotent producer, identifies the producer across multiple sessions)

3. It sends produce requests to (other) brokers and adds partitions and offsets to the transaction on its coordinator.

4. It ends the transaction, either committing it, or rolling it back.

The producer than then repeat steps 3 to 5 indefinitely, or until the producer becomes fenced. 

[[coordinator-discovery,coordinator discovery]]
### Coordinator discovery (`FIND_COORDINATOR`)

NOTE: Coordinator discover is used for both transactional producers and consumer groups, so in this section we'll talk about the client, rather than the producer. We'll refer back to this section later then discussing consumer groups.

Coordinator discovery is about unambiguously identifying a unique broker based on some identity. 
For the transactional producer that identity is its `transactional.id`. 
For a member of a consumer group that identity is its `group.id`.
Let's use the word "subjects" to refer to these coordinator-requiring entities in general. 

1. The subject queries a random broker, giving the required coordinator type and the subject's identity in a <<FindCoordinator>> request.
2. The random broker receives the <<FindCoordinator>> and determines the coordinator according to
+
[source]
----
// Pseudocode
partitionId = abs(hashCode(identity)) mod numPartitions(topicName)
coordinator = leaderOf(topicName, partitionId)
----
+
Where the `topicName` is `\__consumer_offsets` for the consumer group coordinator and `__transaction_state` for the transaction coordinator. 
In other words, the leaders of partitions of those topics have an additional role on top of their leadership.
They have to manage extra broker-side state for the subjects they're coordinating.

3. Client receives <<FindCoordinator>> response and starts talking to this coordinator.

#### `FindCoordinatorRequest`

[id=FindCoordinatorRequest]
[source,javascript]
.The `FIND_COORDINATOR` request
----
include::../kafka-sources/clients/src/main/resources/common/message/FindCoordinatorRequest.json[lines=16..-1]
----

#### `FindCoordinatorResponse`

[id=FindCoordinatorResponse]
[source,javascript]
.The `FIND_COORDINATOR` response
----
include::../kafka-sources/clients/src/main/resources/common/message/FindCoordinatorResponse.json[lines=16..-1]
----

### Obtaining an transactional Producer Id (PID)

This is basically the same `INIT_PRODUCER_ID` API we've seen for the idempotent producer, except:

* it must be send to the transaction coordinator,
* the request includes the `transactional.id` and the transaction timeout

The association between the transactional id and its PID is written to the relevant `__transaction_state` partition by the coordinator.
This allows the same PID to be issued to the same (as identified by transactional id) producer even in the event the leader changes (e.g. due to a broker crash) or the producer process restarts.

These persistent PIDs come with an epoch for fencing zombie producers. 
For example if a new instance of a producer is started, but somehow the old one it still running the request for a PID from the new instance will increment the producer epoch so that RPC from the old one are ingored.

### Beginning a transaction

There is no explicit RPC for starting a transaction. 
The producer API method only changes local state.

### Adding partitions to a transaction

The first time the producer sends a `ProduceRequest` to a new partition (that's not yet part of this transaction) it will also send an `AddPartitionsToTxnRequest` to the coordinator.
The coordinator logs this state so that when the transaction is ended it knows which brokers need to be sent request to write transaction markers.

#### `AddPartitionsToTxnRequest`

[id=AddPartitionsToTxnRequest]
[source,javascript]
.The `ADD_PARTITIONS_TO_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/AddPartitionsToTxnRequest.json[lines=16..-1]
----

#### `AddPartitionsToTxnResponse`

[id=AddPartitionsToTxnResponse]
[source,javascript]
.The `ADD_PARTITIONS_TO_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/AddPartitionsToTxnResponse.json[lines=16..-1]
----

// Example transactional PRODUCE with PID, epoch, sequence number

### Adding offsets to a transaction

Streaming applications (that is, those where records consumed from one set of partitions cause new records to be written other partitions) need a way to add the consumer's offsets to the transaction, since such offset commit also results in appending records to the `__consumer_offsets` partition.

#### `AddOffsetsToTxnRequestRequest`

The `AddOffsetsToTxnRequest` is thus the `__consumer_offsets` flavour of the `AddPartitionsToTxnRequest` for a normal topic, and it sent to the _transaction_ coordinator.

[id=AddOffsetsToTxnRequestRequest]
[source,javascript]
.The `ADD_OFFSETS_TO_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/AddOffsetsToTxnRequest.json[lines=16..-1]
----

#### `AddOffsetsToTxnRequestResponse`

[id=AddOffsetsToTxnRequestResponse]
[source,javascript]
.The `ADD_OFFSETS_TO_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/AddOffsetsToTxnResponse.json[lines=16..-1]
----

#### `TxnOffsetCommitRequest`

The `TxnOffsetCommitRequest` is the `__consumer_offsets` flavour of the `ProduceRequest` for a normal topic and is send to the _group_ coordinator.

[id=TxnOffsetCommitRequest]
[source,javascript]
.The `TXN_OFFSET_COMMIT` request
----
include::../kafka-sources/clients/src/main/resources/common/message/TxnOffsetCommitRequest.json[lines=16..-1]
----

#### `TxnOffsetCommitResponse`

[id=TxnOffsetCommitResponse]
[source,javascript]
.The `TXN_OFFSET_COMMIT` response
----
include::../kafka-sources/clients/src/main/resources/common/message/TxnOffsetCommitResponse.json[lines=16..-1]
----

Transactional producers which are not also consumers don't need to use these RPCs, which are driven use use of the `sendOffsets()` API on the client.

### Ending a transaction

The producer ends the transaction when the application calls `commitTransaction()` or `abortTransaction()`. In either case the producer sends a `EndTxnRequest` to the transaction coordinator. 

#### `EndTxnRequest`

[id=EndTxnRequest]
[source,javascript]
.The `END_TXN` request
----
include::../kafka-sources/clients/src/main/resources/common/message/EndTxnRequest.json[lines=16..-1]
----

#### `EndTxnResponse`

[id=EndTxnResponse]
[source,javascript]
.The `END_TXN` response
----
include::../kafka-sources/clients/src/main/resources/common/message/EndTxnResponse.json[lines=16..-1]
----


#### `WriteTxnMarkersRequest`
The transaction coordinator then uses the accumulated state for the transaction to complete the transaction. 
It does this by sending `WriteTxnMarkersRequests` to each of the brokers leading the partitions added to the transaction, including the `__consumer_offsets` partitions if `sendOffsets()` was used. The transaction markers record whether the transaction was aborted or committed, and will be used later to filter out aborted transactions on consumers with the `read_committed` isolation level.

[id=WriteTxnMarkersRequest]
[source,javascript]
.The `WRITE_TXN_MARKERS` request
----
include::../kafka-sources/clients/src/main/resources/common/message/WriteTxnMarkersRequest.json[lines=16..-1]
----

#### `WriteTxnMarkersResponse`

[id=WriteTxnMarkersResponse]
[source,javascript]
.The `WRITE_TXN_MARKERS` response
----
include::../kafka-sources/clients/src/main/resources/common/message/WriteTxnMarkersResponse.json[lines=16..-1]
----


## The simple Consumer

The simple consumer here means a consumer which will consume from partitions assigned to it via the API ("assigned partitions", cf. "subscribed partitions").

Following the bootstrapping process already described, the simple consumer can start making `FETCH` requests immediately.

### `FETCH`

#### `FetchRequest`

The `FETCH` API is used by both consumers and followers (brokers), and in several different ways.

[id=FetchRequest]
[source,javascript]
.The `FETCH` request
----
include::../kafka-sources/clients/src/main/resources/common/message/FetchRequest.json[lines=16..-1]
----

In the basic case:
1. the consumer makes a fetch request which lists the `Topics/Partition` of interest and the `Topics/Partition/FetchOffset` from which to read
2. it processes the returned data
3. it makes a new fetch request listing (usually) the same partitions, and the offset it computed from the records in the returned data.

That the offset is explicit, which means a fetch request is idempotent.
The broker just zero-copies some data from the log between the requested offset and the last stable offset (LSO) of the log to the network.
In particular, because the broker doesn't really read the data read and it doesn't know how many records were copied, so it doesn't know what the next offset would be.

// TODO example log for a fetch request

#### Incremental fetch 
Most consumers will request data for the same set of partitions repeatedly, meaning that sending `Topics/Partitions` with each request is wasting some network bandwidth.
KIP-XXX added support for "incremental fetch sessions" which avoids this at the cost of some memory on the broker.
The client starts by making an almost ordinatary `FETCH` request, but uses XXX to request the creation of a session.
If the broker allows it the response will include a session id.
Subsequent requests from the client can pass the session id instead of the whole set of partitions.
Or, if the set of partitions changes only the partitions to add or remove from the session need to be included with the request.

// TODO example log for an incremental fetch request

// TODO discuss impl details of the fetch session cache, e.g. limited size, thread contention on the cache.

// TODO limitations of the fetch protocol (e.g. minimal pipelining being a problem on high latency links with high throughput partitions).

#### Consumer follower fetching

NOTE: The term "follower fetch" is ambiguous.
It could refer to the fetching done by a follower, or fetching from a follower by a consumer.
Here we mean the latter.

Usually consumers will fetch from the partition leader, since that will result in the smallest end-to-end latency. 
However, KIP-XXX added support for consumers to fetch from followers. 
This can be beneficial when the consumer is in a different rack than the leader, but the same rack as a follower, and there are significant (financial or performance) costs to fetching between racks.

The basic idea is that the consumer includes `RackId` in its request and the broker's response will include, in addition to the requested data, a `PreferredReadReplica` nominating a broker in that rack to serve future fetch requests. 

#### `FetchResponse`

[id=FetchResponse]
[source,javascript]
.The `FETCH` response
----
include::../kafka-sources/clients/src/main/resources/common/message/FetchResponse.json[lines=16..-1]
----

// TODO discuss some of the fields in the response which are used by the consumer

// TODO OFFSET_COMMIT, OFFSET_FETCH)

## Group protocol

// FIND_COORDINATOR, JOIN_GROUP, SYNC_GROUP, LEAVE_GROUP, HEARTBEAT
Kafka has a separate group _membership_ protocol and group _state_ protocol.

For the consumer, the group state is basically the assignment of group members to partitions.

The group protocol is also used for KafkaConnect task assignment and in Kafka Streams.

This protocol is rather complex, but it allows the client to have control over the group state, rather than having to build this into the broker. The protocol is generic, allowing the same basic protocol to be used for consumers and Kafka Connect, but that means that the coordinator doesn't understand the group state and therefore:

* Cannot validate offsets are being committed by the member which was assigned that partition, requiring that clients are bug-free in this respect.
* Cannot invalidate assignments when the topic's metadata changes, pushing this problem onto the client.

// refer back to coordinator discovery
For each group a single broker, known as the group coordinator, is responsible for managing the group management protocol. Before a client can join a group it needs to discover the group coordinator.

[[group-membership]]
### Group Membership

1. After a client has <<coordinator-discovery,discovered the group coordinator>>, the client sends the coordinator a <<JoinGroup>> request containing metadata about itself and all the group protocols supported by the client.

2. The coordinator waits for up to the rebalance timeout until all it has received a <<JoinGroup>> request from all the members that were in the previous generation (the expected members) 

3. The coordinator selects a protocol which is supported by all members of the group.

4. The coordinator then selects a leader at random and sends a <<JoinGroup>> response in reply to the pending requests. The <<JoinGroup>> response returned to the leader contains the metadata for all the members of the group. The non-leaders are not sent the group membership. In both cases the <<JoinGroup>> response also contains the id of the leader, ??the selected protocol?? and the group generation.
+
A member which joins but doesn't support any protocols supported by the resit of the group gets an error.

5. The leader uses the membership information for the group state protocol.

### Rebalance

Once the client has joined the group it starts sending regular <<Heartbeat>> requests to the coordinator. If the coordinator does not receive a heartbeat from a client before the session timeout the consumer is considered to be dead. The <<Heartbeat>> responses to the remaining clients will have the `REBALANCE_IN_PROGRESS` error code. Those clients have to rejoin the group within the rebalance timeout (measured from the point at which the failed client's session timed out).

In this way a new generation of the group is started. 

<<KIP-62>>.

[[group-state]]
### Group State 

1. The member sends a <<SyncGroup>> request to the coordinator. If the member is the leader of the group this will include the group state. 

2. The coordinator forwards the members state (provided by the leader) in that members <<SyncGroup>> response.

### Offset commit

In order to avoid commits from zombies, <<OffsetCommit>> requests include the group generation id. The coordinator recieves the <<OffsetCommit>> request and validates the generation id. If the generation is not current then the offsets are not committed.

### State machine


TODO: a diagram

The coordinator maintains a state machine with the following states:

`PreparingRebalance`:: The group is preparing to rebalance.
  Transitions to:
  * `CompletingRebalance` if some members join before the timeout.
  * `Empty` if all members have left the group
  * `Dead` if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>).

`CompletingRebalance`:: The group is awaiting for state assignment (<<SyncGroup>>) from the group leader.
  Transitions to:
  * `Stable` if the coordinator receives a <<SyncGroup>> from the group leader.
  * `PreparingRebalance` if the coordinator receives a <<JoinGroup>> from a new member, or an existing member with new metadata; or if the coordinator receives a <<LeaveGroup>> from an existing member; or if member failure is detected (heartbeat timeout)
  * `Dead` if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>).

`Stable`:: Transitions to:
  * `PreparingRebalance` if the coordinator receives a <<JoinGroup>> from a new member, or an existing member with new metadata; or if the coordinator receives a <<LeaveGroup>> from an existing member; or if member failure is detected (heartbeat timeout)

`Dead`:: 
  There are no active members; the group metadata is being cleaned up.
  

`Empty`::
  The group has no members. The group continues in this state until offsets have expired. Transitions to:
  * `Dead` last offsets removed; or if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>); or if the group is removed by expiration.
  * `PreparingRebalnce` if a new member joins the group. 

### Rebalance latency

TODO

### Coordinator crash

The coordinator stores the group generation, leader etc in the group topic (e.g. `__consumer_offsets` for consumer groups). If the coordinator fails that will be detected (for example, by ZooKeeper) and a new leader of the group topic (and thus a new coordinator) will be elected. 

The clients detect failure of the coordinator because they don't receive a <<Heartbeat>> response within the session timeout, so they find the new coordinator and start sending their heartbeats to it. Because the new coordinator can re-read the group topic to discover the membership, generation etc, there is no need for the clients to perform the group membership or state protocols.

### Embedded protocols

How a group works in a client (Consumer, Kafka Connect, Kafka Streams) depends on the  `AbstractCoordinator` implementation, which is responsible for generating all the group protocol requests, and handling all the responses, used by the client. That is, it:

* generates the  <<JoinGroup>> requests (including the necessary use data for each protocol the client can use)
* generates the <<SyncGroup>> request where the leader decides on the group state.

The Kafka Consumer and Kafka Connect use separate `AbstractCoordinator` implementations,`ConsumerCoordinator` and `WorkerCoordinator` respectively, because their group state differs:

* For the consumer it is a partitioning of the subscribed topics
* For a Connect worker it is a partitioning of tasks

`AbstractCoordinator.metadata()` supplies the _protocols_ which get send in the <<JoinGroup>> request.

For the `ConsumerCoordinator` there's another level of pluggability via the `Assignor`. It's at the `Assignor` level that Kafka Streams differs from the plain Kafka Consumer.
For the `ConsumerCoordinator` it's the `Assignors` which correspond to the protocols in the <<JoinGroup>> request.


https://www.youtube.com/watch?v=QaeXDh12EhE

https://cwiki.apache.org/confluence/display/KAFKA/Incremental+Cooperative+Rebalancing%3A+Support+and+Policies#IncrementalCooperativeRebalancing:SupportandPolicies-Motivation

https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol


https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread



## Broker metadata

// LEADER_AND_ISR, UPDATE_METADATA, METADATA_FETCH

The controller broadcasts changes to metadata to other brokers via the
<<UpdateMetadata>> request.

When a broker receives an <<UpdateMetadata>> request it:

* updates its metadata cache, 
* complete any operations which were in purgatory waiting for some change which was present in the new metadata,
* sends a response back to the controller.

When the controller receives an <<UpdateMetadata>> response it basically does nothing (only logs). Thus the logic of the controller does not depend on knowing that a metadata request was received.

ASIDE: I suspect this is because, if a request was not delivered to a broker (e.g. due to crash) then, on restart, the broker will get fresh metadata anyway.

**TODO** How and when does the controller send updatametadata requests

**TODO** Discuss difference between clients and brokers wrt metadata propagation.

## Replication protocol

// TODO follower FETCH and how it differs from consumer fetch

## Reassignment

## Kraft

## Admin requests

// LIST_GROUPS, DESCRIBE_GROUPS
// *_ACLS
// CREATE_/DELETE_TOPICS, CREATE_PARTITIONS
// DELETE_RECORDS
// Other DESCRIBE_*
// KIP-700 DESCRIBE_CLUSTER

# Implementation

//
// Part 2: Implementation
## Producer
### Serialization, partitioning and batching
### Accumulator
### Sender
### TxnManager

.The `TransactionManager` State Machine
[smcat]
....
"Uninit",
Initializing,
Ready,
InTransaction,
CommittingTxn,
AbortingTxn,
AbortableError,
FatalError;

Ready => "Uninit";
"Uninit" => Initializing;
AbortingTxn => Initializing;
Initializing => Ready;
CommittingTxn => Ready;
AbortingTxn => Ready;
Ready => InTransaction;
InTransaction => CommittingTxn;
InTransaction => AbortingTxn;
AbortableError => AbortingTxn;
InTransaction => AbortableError;
CommittingTxn => AbortableError;
AbortableError => AbortableError;
"Uninit" => FatalError;
Initializing => FatalError;
Ready => FatalError;
InTransaction => FatalError;
CommittingTxn => FatalError;
AbortingTxn => FatalError;
AbortableError => FatalError;
....

//
## Consumer
### Fetcher
### `AbstractCoordinator`

.The group `MemberState` state Machine
[smcat]
....
Unjoined,
PreparingRebalance,
CompletingRebalance,
Stable;

Unjoined => PreparingRebalance  : "sent JoinGroup request, but no response yet" ;
PreparingRebalance => CompletingRebalance : "received JoinGroup response, but no assignment yet" ;
CompletingRebalance => Stable : "Joined; sending heartbeats" ;
....

//
## Broker
### Startup
### Shutdown
#### Controller shutdown
#### Full disks
#### Crash
### ReplicaManager
### RSM
### GroupCoordinator

.The group `GroupState` state Machine
[smcat]
....
PreparingRebalance,
CompletingRebalance,
Stable,
Dead,
Empty;

Stable => PreparingRebalance : "multiple reasons";
CompletingRebalance => PreparingRebalance : "leave group from existing member, or member failure detected";
Empty => PreparingRebalance;

PreparingRebalance => CompletingRebalance : "members joined in time";
CompletingRebalance => Stable : "sync group with assignment received";

Stable => Dead : "emigration";
PreparingRebalance => Dead : "emigration";
CompletingRebalance => Dead : " emigration";
Empty => Dead : "offset expiration";
Dead => Dead;

PreparingRebalance => Empty : "all members left";
....


.The Replica State Machine
[smcat]
....
NewReplica,
OnlineReplica,
OfflineReplica,
ReplicaDeletionStated,
ReplicaDeletionSuccessful,
ReplicaDeletionIneligible,
NonexistentReplica;

NewReplica                => OnlineReplica             : "in assigned replicas";
NewReplica                => OfflineReplica            : "hosting broker dies";
OnlineReplica             => OfflineReplica            : "hosting broker dies";
OnlineReplica             => OnlineReplica             ;
OfflineReplica            => OnlineReplica             ;
OfflineReplica            => OfflineReplica            ;
OfflineReplica            => ReplicaDeletionStated     : "deletion started";
ReplicaDeletionStated     => ReplicaDeletionSuccessful : "no error";
ReplicaDeletionStated     => ReplicaDeletionIneligible : "error";
OfflineReplica            => ReplicaDeletionIneligible ;
ReplicaDeletionIneligible => OnlineReplica             ;
ReplicaDeletionIneligible => OfflineReplica            : "hosting broker dies";
ReplicaDeletionSuccessful => NonexistentReplica        : "deleted";
NonexistentReplica        => NewReplica                : "created";
....

### PSM

.The Partition State Machine
[smcat]
....
initial,
 NonExistentPartition,
 NewPartition,
 OnlinePartition,
 OfflinePartition;

initial              => NonExistentPartition;
OfflinePartition     => NonExistentPartition : "deleted";
NonExistentPartition => NewPartition         : "created";
NewPartition         => OnlinePartition      : "leader elected";
OfflinePartition     => OnlinePartition      : "new leader elected";
NewPartition         => OfflinePartition;
OnlinePartition      => OfflinePartition     : "leader dies";
....

### Logs and segments
### TxnCoordinator
### GroupCoordinator
//
##  Controller
//
## Kafka Connect
//
## Kafka Streams
//
// Part 3: Operations
//
// Appendices
//   KIP index
//   RPC index
//   Index of message schemas (both public and internal)

// Very old content which should be rewritten
//
// include::authentication.adoc[]

// include::producer.adoc[]

// include::consumer.adoc[]
