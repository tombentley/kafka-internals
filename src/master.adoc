// asciidoctor -r asciidoctor-diagram master .adoc

# Kafka internals
Tom Bentley
:toc: left  
:toclevels: 4
:source-highlighter: pygments


:leveloffset: +1

This document is about the _implementation_ of Apache Kafka.
It does not address why you might want to use Kafka, or how to do anything in particular with it.
Instead it discusses both the protocol and the code in depth. 
It is largely aimed as answering the question "How does Kafka work?".
It assumes you have some familiarity with Kafka concepts such as topics, partitions, replicas, logs and segments.

The content is broken into three parts. 
First the Kafka protocol is introduced, mostly without talking about implementation details.
Second we talk about the implementation of the Apache Kafka clients, broker and other things.
Finally we try to put this theoretical knowledge to use by talking about how it relates to real problems.


# The Kafka Protocol

In this part we cover most of the Kafka protocol in the abstract.
We're not concerned here with how messages get serialized, networking, threading or any other implementation concerns.
This is purely about what information flows between which peers, and what they might do with it.

## APIs

Kafka's is almost uniformly a request-response protocol that runs on TCP or TLS transports. 
Request and responses are sometimes collectively called "messages", or RPCs (remote procedure calls). "Messages" is a little confusing since records are also often called "messages", so we'll use the term RPC instead.

Each request includes a number, called the API key, which identifies the type of request. For example, the produce request, used to send records to a broker, has API key 0, and the fetch request has the API key 1.

RPCs consist of a header and a body. 

### Request header
For requests the header looks like this:

[id=RequestHeader]
[source,javascript]
.The request header
----
include::../kafka-sources/clients/src/main/resources/common/message/RequestHeader.json[lines=16..-1]
----

We'll discuss the `RequestApiVersion` shortly. 
The `CorrelationId` allows brokers to respond to requests in a different order than the client sent them in: 
The client uses a unique id for each request, which is included in the response, and the client matches the response to the request using the id.

### Response header
Responses have their own header:

[id=ResponseHeader]
[source,javascript]
.The response header
----
include::../kafka-sources/clients/src/main/resources/common/message/ResponseHeader.json[lines=16..-1]
----

// TODO define API key
// TODO request response (almost always)
// TODO request header
// TODO response header

## Participants
// define client, broker, producer, consumer, follower, leader, controller, zk

Let's define the participants in the Kafka protocol.

### Clients

The applications which talk to a cluster from the outside are _clients_. These include _producers_, which send messages to the cluster to be appended to logs, and _consumers_ which read from those logs. It also includes things like the _admin_ client, which is used to manage the cluster. Although Apache Kafka packages its clients as three separate classes this distinction is not meaningful at the protocol level. What defines a client is its use of the client-facing APIs (often referred to as RPCs).

### Brokers

Clients usually talk to _brokers_, which are network server processes with the main purpose of storing logs. When talking about a broker in relation to a particular partition it may be acting in the _leader_ or _follower_ role. In other words, a _leader_ or _follower_ is a broker acting in a certain role with respect to a given partition.

### Controller

The _controller_ is a network server process with the task of coordinating the metadata necessary for the operation of the cluster. 
In ZK-mode the controller is broker which takes on the controller role (for a time).
In Kraft mode the controller is _usually_ one of a separate set of network servers making up the raft cluster, though the raft cluster may instead be made up of a subset of the brokers in smaller, typically non-production, deployments.

### Kraft nodes

### ZooKeeper nodes

### Servers

This document will use the term "server" to collectively refer to something that could be a broker, a kraft node or possibly a zookeeper node and a more specific term would be incorrect. For example, both brokers and kraft nodes support the METADATA RPC, so "server" would be the appropriate term for the recipient of a metadata request.

## Client bootstrapping
// API_VERSIONS, SASL*, METADATA

When a client starts up it has no existing knowledge of the cluster state, it only has the set of "bootstrap servers" with which it has been configured.
Bootstrapping is the process of connecting to one of those servers and discovering the necessary cluster state in order to perform whatever that client is supposed to do. The necessary cluster state includes the available brokers and their addresses and possibly information about topics of interest.

In order to do this it needs:

1. To know how to talk to the bootstrap broker it connects to. 
The Kafka RPCs are individually versioned, so in order to send requests that the server will understand the Apache Kafka clients need to know the versions supported by the server.
This the `API_VERSIONS` request allows a client to discover this.
By sending a `API_VERSIONS` request first, a client can determine which versions of all the other RPCs it should use by taking `max(intersection(supported_client_versions, supported_broker_versions))` for that particular API.

2. Possibly to authenticate to the server. The `API_VERSIONS` request is the only one which a listener requiring authentication will handle without requiring authentication of its peer. 
If TLS is used for authentication then the server will already know the client's identity as a result of the TLS handshake. 
If SASL is being used then `SASL_HANDSHAKE` and `SASL_AUTHENTICATE` are exchanged.

3. To discover information about the cluster, including the currently live brokers and information about certain topics of interest to the client.
This is done using `METADATA` requests.
One response with the information about the live brokers is enough to bootstrap the client, but clients typically make more later as the state of the client and cluster changes. 

The process described above is common to all _Apache_ Kafka clients.
Other client libraries sometimes work slightly differently.
For example, a Sarama client has to be configured with the version of the broker it is talking to and uses a hard-coded with the mapping from "broker version" to supported API versions. 

In the following sections we'll explore these requests in detail.

### `APIVersions`

#### `ApiVersionsRequest`

The API_VERSIONS request is trivial, having only a couple of optional fields for telling the server the name and version of the client library.
// TODO for each request: required authz, idempotency
[id=ApiVersionsRequest]
[source,javascript]
.The API_VERSIONS request
----
include::../kafka-sources/clients/src/main/resources/common/message/ApiVersionsRequest.json[lines=16..-1]
----

#### `ApiVersionsResponse`

The response is essentially composed of a object for each API/RPC key, containing the maximum and minimum version supported by the broker. 
[id=ApiVersionsResponse]
[source,javascript]
.The API_VERSIONS response
----
include::../kafka-sources/clients/src/main/resources/common/message/ApiVersionsResponse.json[lines=16..-1]
----

// client doesn't know broker version
NOTE:: The API versions response does not include a server software version, e.g. that a broker is running Kafka 3.0. 
This is intentional as it discourages clients from becoming dependent on the broker version (e.g. using it to work around bugs).
The rationale is that it's better that the broker be fixed by a software update than for clients implement version-specific behaviour.
Of course this does not prevent the set of RPCs and their maximum and minimum versions being used to infer the broker version.

// Feature gates
KIP-584 added support for 'feature gates', which is a mechanism for versioning features which apply at the cluster level (rather than per-broker API versioning).
The supported and enabled features are also included.
Features are only supported when all servers in a cluster support them, and are enabled by an administrator.

### SASL authentication

// Example using SASL plain

// Example using SASL SCRAM-SHA

// Example using OAUTH_BEARER

### Metadata requests

// The initial cluster request

// Topic requests

// Topic creation by side-effect

// authorized operations

// Refreshing metadata

## The Producer

Having done the necessary bootstrapping and discovered which brokers are acting as leader for the relevant partitions, a producer is finally in a position to send messages.
This is done using a `PRODUCE` request which essentially contains the messages to be appended to the log for some set of partitions led by that broker.

### `PRODUCE`

#### `ProduceRequest`

[id=ProduceRequest]
[source,javascript]
.The `PRODUCE` request
----
include::../kafka-sources/clients/src/main/resources/common/message/ProduceRequest.json[lines=16..-1]
----

#### `ProduceResponse`

When the produce request has `acks=1` or `acks=all` the broker will response with a `ProduceResponse` once all the records have been appended to its, and for `acks=all` enough follower, logs.

NOTE: `acks=0` is a special case in the Kafka protocol: It's the only request which doesn't result in a response being returned.
That is, `acks=0` doesn't just mean the producer doesn't _wait_ for an acknowledgement, it means there is not acknowledgement at all.

[id=ProduceResponse]
[source,javascript]
.The `PRODUCE` response
----
include::../kafka-sources/clients/src/main/resources/common/message/ProduceResponse.json[lines=16..-1]
----

Of course there's the possibility that the broker is not the leader for some of the partitions in the request.
In this case the XXXX error will prompt the producer to refresh it's metadata and resend the records for those partitions to the new leader, if there is one. 
If there is no new leader the messages will eventually expire.

// Idempotency

### The record format
// Record format

## Transactional produce

KIP-98 added support for transactions. 
This allows a producer to send messages to a set of partitions (with leaders on multiple brokers) with a transaction that is committed atomically.
"Atomically" means either all the messages in the transaction become visible to consumers or none of them do.
To enable this the producer uses a number of additional requests.

1. It must find the broker which is acting as its transaction coordinator.

2. It gets a producer id, which allows it to create transactions. 

3. It begins a transaction with that coordinator.

4. It sends produce requests to (other) brokers and adds partitions and offsets to the transaction on its coordinator.

5. It ends the transaction, either committing it, or rolling it back.

The producer than then repeat steps 3 to 5 indefinitely, or until the producer becomes fenced. 

[[coordinator-discovery,coordinator discovery]]
### Coordinator discovery (`FIND_COORDINATOR`)

NOTE: Coordinator discover is used for both transactional producers and consumer groups, so in this section we'll talk about the client, rather than the producer. We'll refer back to this section later then discussing consumer groups.

Coordinator discovery is about unambiguously identifying a unique broker based on some identity. 
For the transactional producer that identity is its `transactional.id`. 
For a member of a consumer group that identity is its `group.id`.
Let's use the word "subjects" to refer to these coordinator-requiring entities in general. 

1. The subject queries a random broker, giving the required coordinator type and the subject's identity in a <<FindCoordinator>> request.
2. The random broker receives the <<FindCoordinator>> and determines the coordinator according to
+
[source]
----
// Pseudocode
partitionId = abs(hashCode(identity)) mod numPartitions(topicName)
coordinator = leaderOf(topicName, partitionId)
----
+
Where the `topicName` is `__consumer_offsets` for the consumer group coordinator and `__transaction_state` for the transaction coordinator. 
In other words, the leaders of partitions of those topics have an additional role on top of their leadership.
They have to manage extra broker-side state for the subjects they're coordinating.

3. Client receives <<FindCoordinator>> response and starts talking to this coordinator.

#### `FindCoordinatorRequest`

[id=FindCoordinatorRequest]
[source,javascript]
.The `FIND_COORDINATOR` request
----
include::../kafka-sources/clients/src/main/resources/common/message/FindCoordinatorRequest.json[lines=16..-1]
----

#### `FindCoordinatorResponse`

[id=FindCoordinatorResponse]
[source,javascript]
.The `FIND_COORDINATOR` response
----
include::../kafka-sources/clients/src/main/resources/common/message/FindCoordinatorResponse.json[lines=16..-1]
----

### Obtaining a Producer Id (PID)

### Beginning a transaction

### Adding partitions to a transaction

### Adding offsets to a transaction

### Ending a transaction

## Consumer Fetch

// (including incremental fetch) (consumer FETCH, OFFSET_COMMIT, OFFSET_FETCH)

## Group protocol

// FIND_COORDINATOR, JOIN_GROUP, SYNC_GROUP, LEAVE_GROUP, HEARTBEAT
Kafka has a separate group _membership_ protocol and group _state_ protocol.

For the consumer, the group state is basically the assignment of group members to partitions.

The group protocol is also used for KafkaConnect task assignment and in Kafka Streams.

This protocol is rather complex, but it allows the client to have control over the group state, rather than having to build this into the broker. The protocol is generic, allowing the same basic protocol to be used for consumers and Kafka Connect, but that means that the coordinator doesn't understand the group state and therefore:

* Cannot validate offsets are being committed by the member which was assigned that partition, requiring that clients are bug-free in this respect.
* Cannot invalidate assignments when the topic's metadata changes, pushing this problem onto the client.

// refer back to coordinator discovery
For each group a single broker, known as the group coordinator, is responsible for managing the group management protocol. Before a client can join a group it needs to discover the group coordinator.

[[group-membership]]
### Group Membership

1. After a client has <<coordinator-discovery,discovered the group coordinator>>, the client sends the coordinator a <<JoinGroup>> request containing metadata about itself and all the group protocols supported by the client.

2. The coordinator waits for up to the rebalance timeout until all it has received a <<JoinGroup>> request from all the members that were in the previous generation (the expected members) 

3. The coordinator selects a protocol which is supported by all members of the group.

4. The coordinator then selects a leader at random and sends a <<JoinGroup>> response in reply to the pending requests. The <<JoinGroup>> response returned to the leader contains the metadata for all the members of the group. The non-leaders are not sent the group membership. In both cases the <<JoinGroup>> response also contains the id of the leader, ??the selected protocol?? and the group generation.
+
A member which joins but doesn't support any protocols supported by the resit of the group gets an error.

5. The leader uses the membership information for the group state protocol.

### Rebalance

Once the client has joined the group it starts sending regular <<Heartbeat>> requests to the coordinator. If the coordinator does not receive a heartbeat from a client before the session timeout the consumer is considered to be dead. The <<Heartbeat>> responses to the remaining clients will have the `REBALANCE_IN_PROGRESS` error code. Those clients have to rejoin the group within the rebalance timeout (measured from the point at which the failed client's session timed out).

In this way a new generation of the group is started. 

<<KIP-62>>.

[[group-state]]
### Group State 

1. The member sends a <<SyncGroup>> request to the coordinator. If the member is the leader of the group this will include the group state. 

2. The coordinator forwards the members state (provided by the leader) in that members <<SyncGroup>> response.

### Offset commit

In order to avoid commits from zombies, <<OffsetCommit>> requests include the group generation id. The coordinator recieves the <<OffsetCommit>> request and validates the generation id. If the generation is not current then the offsets are not committed.

### State machine


TODO: a diagram

The coordinator maintains a state machine with the following states:

`PreparingRebalance`:: The group is preparing to rebalance.
  Transitions to:
  * `CompletingRebalance` if some members join before the timeout.
  * `Empty` if all members have left the group
  * `Dead` if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>).

`CompletingRebalance`:: The group is awaiting for state assignment (<<SyncGroup>>) from the group leader.
  Transitions to:
  * `Stable` if the coordinator receives a <<SyncGroup>> from the group leader.
  * `PreparingRebalance` if the coordinator receives a <<JoinGroup>> from a new member, or an existing member with new metadata; or if the coordinator receives a <<LeaveGroup>> from an existing member; or if member failure is detected (heartbeat timeout)
  * `Dead` if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>).

`Stable`:: Transitions to:
  * `PreparingRebalance` if the coordinator receives a <<JoinGroup>> from a new member, or an existing member with new metadata; or if the coordinator receives a <<LeaveGroup>> from an existing member; or if member failure is detected (heartbeat timeout)

`Dead`:: 
  There are no active members; the group metadata is being cleaned up.
  

`Empty`::
  The group has no members. The group continues in this state until offsets have expired. Transitions to:
  * `Dead` last offsets removed; or if the coordinator ceases to be leader for the coordinating partition (see <<coordinator-discovery>>); or if the group is removed by expiration.
  * `PreparingRebalnce` if a new member joins the group. 

### Rebalance latency

TODO

### Coordinator crash

The coordinator stores the group generation, leader etc in the group topic (e.g. `__consumer_offsets` for consumer groups). If the coordinator fails that will be detected (for example, by ZooKeeper) and a new leader of the group topic (and thus a new coordinator) will be elected. 

The clients detect failure of the coordinator because they don't receive a <<Heartbeat>> response within the session timeout, so they find the new coordinator and start sending their heartbeats to it. Because the new coordinator can re-read the group topic to discover the membership, generation etc, there is no need for the clients to perform the group membership or state protocols.

### Embedded protocols

How a group works in a client (Consumer, Kafka Connect, Kafka Streams) depends on the  `AbstractCoordinator` implementation, which is responsible for generating all the group protocol requests, and handling all the responses, used by the client. That is, it:

* generates the  <<JoinGroup>> requests (including the necessary use data for each protocol the client can use)
* generates the <<SyncGroup>> request where the leader decides on the group state.

The Kafka Consumer and Kafka Connect use separate `AbstractCoordinator` implementations,`ConsumerCoordinator` and `WorkerCoordinator` respectively, because their group state differs:

* For the consumer it is a partitioning of the subscribed topics
* For a Connect worker it is a partitioning of tasks

`AbstractCoordinator.metadata()` supplies the _protocols_ which get send in the <<JoinGroup>> request.

For the `ConsumerCoordinator` there's another level of pluggability via the `Assignor`. It's at the `Assignor` level that Kafka Streams differs from the plain Kafka Consumer.
For the `ConsumerCoordinator` it's the `Assignors` which correspond to the protocols in the <<JoinGroup>> request.


https://www.youtube.com/watch?v=QaeXDh12EhE

https://cwiki.apache.org/confluence/display/KAFKA/Incremental+Cooperative+Rebalancing%3A+Support+and+Policies#IncrementalCooperativeRebalancing:SupportandPolicies-Motivation

https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol


https://cwiki.apache.org/confluence/display/KAFKA/KIP-62%3A+Allow+consumer+to+send+heartbeats+from+a+background+thread



## Broker metadata

// LEADER_AND_ISR, UPDATE_METADATA, METADATA_FETCH

## Replication protocol

// FETCH

## Reassignment

## Kraft

## Admin requests

// LIST_GROUPS, DESCRIBE_GROUPS
// *_ACLS
// CREATE_/DELETE_TOPICS, CREATE_PARTITIONS
// DELETE_RECORDS
// Other DESCRIBE_*
// KIP-700 DESCRIBE_CLUSTER

# Implementation

//
// Part 2: Implementation
## Producer
### Serialization, partitioning and batching
### Accumulator
### Sender
### TxnManager

.The `TransactionManager` State Machine
[smcat]
....
"Uninit",
Initializing,
Ready,
InTransaction,
CommittingTxn,
AbortingTxn,
AbortableError,
FatalError;

Ready => "Uninit";
"Uninit" => Initializing;
AbortingTxn => Initializing;
Initializing => Ready;
CommittingTxn => Ready;
AbortingTxn => Ready;
Ready => InTransaction;
InTransaction => CommittingTxn;
InTransaction => AbortingTxn;
AbortableError => AbortingTxn;
InTransaction => AbortableError;
CommittingTxn => AbortableError;
AbortableError => AbortableError;
"Uninit" => FatalError;
Initializing => FatalError;
Ready => FatalError;
InTransaction => FatalError;
CommittingTxn => FatalError;
AbortingTxn => FatalError;
AbortableError => FatalError;
....

//
## Consumer
### Fetcher
### `AbstractCoordinator`

.The group `MemberState` state Machine
[smcat]
....
Unjoined,
PreparingRebalance,
CompletingRebalance,
Stable;

Unjoined => PreparingRebalance  : "sent JoinGroup request, but no response yet" ;
PreparingRebalance => CompletingRebalance : "received JoinGroup response, but no assignment yet" ;
CompletingRebalance => Stable : "Joined; sending heartbeats" ;
....

//
## Broker
### Startup
### Shutdown
#### Controller shutdown
#### Full disks
#### Crash
### ReplicaManager
### RSM
### GroupCoordinator

.The group `GroupState` state Machine
[smcat]
....
PreparingRebalance,
CompletingRebalance,
Stable,
Dead,
Empty;

Stable => PreparingRebalance : "multiple reasons";
CompletingRebalance => PreparingRebalance : "leave group from existing member, or member failure detected";
Empty => PreparingRebalance;

PreparingRebalance => CompletingRebalance : "members joined in time";
CompletingRebalance => Stable : "sync group with assignment received";

Stable => Dead : "emigration";
PreparingRebalance => Dead : "emigration";
CompletingRebalance => Dead : " emigration";
Empty => Dead : "offxet expiration";
Dead => Dead;

PreparingRebalance => Empty : "all members left";
....


.The Replica State Machine
[smcat]
....
NewReplica,
OnlineReplica,
OfflineReplica,
ReplicaDeletionStated,
ReplicaDeletionSuccessful,
ReplicaDeletionIneligible,
NonexistentReplica;

NewReplica                => OnlineReplica             : "in assigned replicas";
NewReplica                => OfflineReplica            : "hosting broker dies";
OnlineReplica             => OfflineReplica            : "hosting broker dies";
OnlineReplica             => OnlineReplica             ;
OfflineReplica            => OnlineReplica             ;
OfflineReplica            => OfflineReplica            ;
OfflineReplica            => ReplicaDeletionStated     : "deletion started";
ReplicaDeletionStated     => ReplicaDeletionSuccessful : "no error";
ReplicaDeletionStated     => ReplicaDeletionIneligible : "error";
OfflineReplica            => ReplicaDeletionIneligible ;
ReplicaDeletionIneligible => OnlineReplica             ;
ReplicaDeletionIneligible => OfflineReplica            : "hosting broker dies";
ReplicaDeletionSuccessful => NonexistentReplica        : "deleted";
NonexistentReplica        => NewReplica                : "created";
....

### PSM

.The Partition State Machine
[smcat]
....
initial,
 NonExistentPartition,
 NewPartition,
 OnlinePartition,
 OfflinePartition;

initial              => NonExistentPartition;
OfflinePartition     => NonExistentPartition : "deleted";
NonExistentPartition => NewPartition         : "created";
NewPartition         => OnlinePartition      : "leader elected";
OfflinePartition     => OnlinePartition      : "new leader elected";
NewPartition         => OfflinePartition;
OnlinePartition      => OfflinePartition     : "leader dies";
....

### Logs and segments
### TxnCoordinator
### GroupCoordinator
//
##  Controller
//
## Kafka Connect
//
## Kafka Streams
//
// Part 3: Operations
//
// Appendices
//   KIP index
//   RPC index
//   Index of message schemas (both public and internal)


// Very old content which should be rewritten
//
// include::authentication.adoc[]

// // TODO include::authorization.adoc[]

// include::producer.adoc[]

// include::group-protocol.adoc[]

// include::consumer.adoc[]

// // TODO transactions

// // TODO include::admin-client.adoc[]

// include::metadata.adoc[]

// include::broker.adoc[]

// // TODO logs and segments

// // TODO include::controller.adoc[]

// // TODO partition reassignment

// // Kafka connect (and its embedded protocol)

// // Kafka Streams (and its embedded protocol)

// include::protocol.adoc[]



// include::bibliography.adoc[]