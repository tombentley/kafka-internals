= Kafka internals: Client bootstrapping
:source-highlighter: highlightjs
:icons: font
:revealjs_hash: true
:customcss: pres.css

## Motivating questions

* What does a producer need to do before it can start sending `PRODUCE` requests?
* I saw my client trying to connect to `node=-2` in the logs. My cluster has no such broker! What does it mean?

## Participants
// define client, broker, producer, consumer, follower, leader, controller, zk

Let's define the participants in the Kafka protocol.

### Clients

* The applications which talk to a cluster from the outside are _clients_.
* What defines a client is its use of the client-facing APIs.
* These include _producers_, which send messages to the cluster to be appended to logs, and _consumers_ which read from those logs.
* It also includes things like the _admin_ client, which is used to manage the cluster.

### Brokers

* _Brokers_ are network server processes with the main purpose of storing logs. 
* When talking about a broker in relation to a particular partition it may be acting in the _leader_ or _follower_ (or neither) role.
* A broker may be leader for some partitions, and follower for others.
* Each partition has 0 or 1 leader

### Controller

* The _controller_ is a network server process with the task of coordinating the metadata necessary for the operation of the cluster. 
* In ZK-mode the controller is a broker which takes on the controller role (for a time).
* In KRaft mode the controller is _usually_ one of a separate set of network servers making up the raft cluster. 
  (In non-production deployments the raft cluster may instead be made up of a subset of the brokers).

### Kraft nodes

### ZooKeeper nodes

### Diagram

TODO picture

## API/RPCs

* Kafka's is almost uniformly a request-response protocol that runs on TCP or TLS transports.

* Requests and responses consist of a header and a body. 

* Each request includes a number, called the API key, which identifies the type of request.

* For example, the produce request, used to send records to a broker, has API key 0.

### Request header
For requests, the header has this schema:

[.stretch]
[id=RequestHeader]
[source,javascript]
----
include::../kafka-sources/clients/src/main/resources/common/message/RequestHeader.json[lines=16..-1]
----


### Response header
Responses have their own header, which just includes the correlation id.

[.stretch]
[id=ResponseHeader]
[source,javascript]
----
include::../kafka-sources/clients/src/main/resources/common/message/ResponseHeader.json[lines=16..-1]
----

### Throttling

Although it's not part of the response header, most responses have a top-level field in the body called `ThrottleTimeMs`, which we'll discuss here.
Brokers need to be able to throttle clients but they can't control how quickly clients send requests.
Therefore the only mechanism the broker can use to throttle clients is to not return a response immediately when the client's quota is exceeded.
This would be completely transparent to the client, so the `ThrottleTimeMs` field is used to inform the client that the response was delayed for the given amount of time.
A sufficiently sophisticated client could use this to slow the rate it sends RPCs to that broker.
This would make most sense for those requests which can be sent to any broker.


## Client bootstrapping
// API_VERSIONS, SASL*, METADATA

* When a client starts up it has no existing knowledge of the cluster state, it only has the set of "bootstrap servers" with which it has been configured.
* Bootstrapping is the process of connecting to one of those servers and discovering the necessary cluster state in order to perform whatever that client is supposed to do. 
* The necessary cluster state includes the available brokers and their addresses and possibly information about topics of interest.

### !
In order to do this it needs:

1. To know how to talk to the bootstrap broker it connects to. 

2. Possibly to authenticate to the server. 

3. To discover information about the cluster, including the currently live brokers and information about certain topics of interest to the client.

(Other client libraries sometimes work slightly differently.)

// TODO logs showing connection to broker -1

## API versions

* The Kafka RPCs are individually versioned

* The `API_VERSIONS` request allows a client to discover the versions supported by the broker.

* This enabled forward and backward compatibility

* (Most) clients send this first.

* `max(intersection(supported_client_versions, supported_broker_versions))` for a given API.

### Example ApiVersions Request

(from broker logs)
[source]
....
[2021-04-30 10:04:53,276] TRACE [KafkaApi-0] Handling request:
RequestHeader(apiKey=API_VERSIONS, apiVersion=3, 
clientId=console-producer, correlationId=0) -- <1>
ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', 
clientSoftwareVersion='2.9.0-SNAPSHOT') from connection  <2>
192.168.178.20:9092-192.168.178.20:34198-0;securityProtocol:
PLAINTEXT,principal:User:ANONYMOUS (kafka.server.KafkaApis)
....
<1> 1st request from this client
<2> It's a AK client

### Example ApiVersions Response

(from client logs)

[source]
....
2021-04-30 10:22:03,613] DEBUG [Producer clientId=console-producer] 
Received API_VERSIONS response from node -1 for request with header <1>
RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=console-producer, 
correlationId=0): ApiVersionsResponseData(errorCode=0, apiKeys=[
    ApiVersion(apiKey=0, minVersion=0, maxVersion=9), <2>
... <3>
throttleTimeMs=0, <4>
supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[])
....
<1> node -1 means it's first `bootstrap.server`
<2> Support versions 0..9 of `PRODUCE` API
<3> Lots more `ApiVersions`
<4> Response wasn't throttled

## Authentication

* If TLS is used for authentication then no need for SASL. 

* The `API_VERSIONS` request is the only one which a listener requiring authentication will handle without requiring authentication of its peer. 

* If SASL is being used then `SASL_HANDSHAKE` and `SASL_AUTHENTICATE` are exchanged.

### !

1. The client sends a `SASL_HANDSHAKE` request to try to use a SASL mechanism.

2. If the broker supports that mechanism the client follows with one or more `SASL_AUTHENTICATE` requests wrapping the SASL frames.

3. `SASL_HANDSHAKE` v0 didn't use `SASL_AUTHENTICATE`, but sent non-wrapped SASL frames.

### Example successful auth

### Example failed auth

## Metadata

The client now needs to find out about the rest of the cluster.

* Broker responds with the information about the _live_ brokers
* Clients make more requests later as the state of the client and cluster changes. 
* Very flexible API.

### Client metadata caches

The Apache Kafka clients have a cache of the metadata and will refresh it when:

* they observe a response which an error code which implies that the metadata is out of date
* periodically, if it's not otherwise been refreshed

* In the admin client, the user visible methods such as `describeCluster()`, `listTopics()` and `describeTopics()` are separated from the cache, and always make a fresh request.

### Example metadata request

[source]
....
[2021-04-30 10:04:53,296] TRACE [KafkaApi-0] Handling request:RequestHeader(apiKey=METADATA, 
apiVersion=11, clientId=console-producer, correlationId=7) -- MetadataRequestData(
    topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='foo')], 
    allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, <1>
    includeTopicAuthorizedOperations=false) from connection 
    192.168.178.20:9092-192.168.178.20:34198-0;securityProtocol:PLAINTEXT,
    principal:User:ANONYMOUS (kafka.server.KafkaApis)
....
<1> allow topic creation because this is from a producer

### Example metadata response

[source]
....
[2021-04-30 10:04:53,324] TRACE [KafkaApi-0] Sending topic metadata 
MetadataResponseTopic(errorCode=5, name='foo', topicId=AAAAAAAAAAAAAAAAAAAAAA, <1>
isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)  <2>
and brokers MetadataBroker(0,null,Map(
    PLAINTEXT -> thinkpad-p50.fritz.box:9092 (id: 0 rack: null)),false) 
    for correlation id 7 to client console-producer (kafka.server.KafkaApis)
....
<1> 5=LEADER_NOT_AVAILABLE (topic created by side-effect)
<2> no partitions because topic is still being created


## So far...

Typical successful client bootstrapping

[source]
----
C: ApiVersions v3
S: ApiVersions v3 (ErrorCode=0,
    ApiKeys=[{ApiKey=SaslHandshake, Min=0, Max=1},
            {ApiKey=SaslAuthenticate, Min=0, Max=2},
            {ApiKey=Metadata, Min=0, Max=11}, ...])
C: SaslHandshake v1 (Mechanism=plain)
S: SaslHandshake v1 (ErrorCode=0)
C: SaslAuthenticate v2 (
    sasl_auth_bytes=<NUL>tom<NUL>asdf1234)
S: SaslAuthenticate v2 (ErrorCode=0, 
    sasl_auth_bytes=) // empty
C: Metadata v11 ()
S: Metadata v11 (ErrorCode=0, Brokers=...)
----

### ...What next?

* So at this point the client knows about the live brokers in the cluster
* When it connects to then it will need to perform some of the same requests
  - ApiVersions
  - SaslHandShake and SaslAuthenticate
* It will need to do this every time it reconnects to that broker too?
  Because, in general, a disconnection could be due to a broker restart during an upgrade.
  And brokers might not all be running the same version.

## Summary

* We've covered how clients (producers, consumer and admin clients) make their initial connections to the cluster.

### Answers

What does a producer need to do before it can start sending `PRODUCE` requests?::
Discover the whole of the cluster, authenticate to brokers, know how to talk to them.

I saw my client trying to connect to `node=-2` in the logs. My cluster has no such broker! What does it mean?::
It's trying to connect to the 2nd bootstrap broker in it's list.